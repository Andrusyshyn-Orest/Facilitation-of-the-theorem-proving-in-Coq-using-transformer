{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "4H5T74YvTGQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import notebook_login, Repository\n",
        "from google.colab import drive\n",
        "import json"
      ],
      "metadata": {
        "id": "qfIvzG7b0u37"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMS AND GLOBAL VARS\n",
        "vocab_size                     = 30000                             # vocabulary size of the tokenizer\n",
        "base_model                     = \"gpt2\"                            # base model Hugging Face name\n",
        "batch_size                     = 20\n",
        "\n",
        "drive_mounted                  = False\n",
        "drive_mounted_path             = \"/content/gdrive/\"                # only needed if drive_mounted == True\n",
        "train_data_archived            = True                              # set to True if you need to extract the data from archive\n",
        "raw_train_archive              = \"./dataset_train.zip\"             # only needed if data_archived == True\n",
        "raw_train_json                 = \"./dataset_train.json\"            # path to the train dataset (after extracting it will be in the current working directory)\n",
        "\n",
        "config_file                    = None\n",
        "\n",
        "push_to_hub                    = False                             # set to True if you need to commit changes\n",
        "user_email                     = \"user_email\"                      # only needed if push_to_hub == True\n",
        "user_name                      = \"user_name\"                       # only needed if push_to_hub == True\n",
        "tokenizer_repo_name            = \"Andrusyshyn/gpt2-coq-tokenizer\"  # only needed if push_to_hub == True\n",
        "tokenizer_output_dir           = \"gpt2-coq-tokenizer_local\"        # local dir to save the tokenizer\n",
        "run_name                       = \"experimental\"                    # branch name (only needed if push_to_hub == True)"
      ],
      "metadata": {
        "id": "RCwNJhLe0MdL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_config(config_file: str):\n",
        "    \"\"\"\n",
        "    Parses config_file and sets global variables.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config_file : str\n",
        "        path to config file.\n",
        "    \"\"\"\n",
        "    global vocab_size, base_model, batch_size, raw_train_json, push_to_hub,\\\n",
        "        tokenizer_repo_name, tokenizer_output_dir, run_name, drive_mounted,\\\n",
        "        drive_mounted_path, train_data_archived, raw_train_archive, user_email, user_name\n",
        "\n",
        "    with open(config_file, mode='r') as conf_file:\n",
        "        conf_data = json.load(conf_file)\n",
        "\n",
        "    vocab_size                     = conf_data[\"vocab_size\"]\n",
        "    base_model                     = conf_data[\"base_model\"]\n",
        "    batch_size                     = conf_data[\"batch_size\"]\n",
        "\n",
        "    raw_train_json                 = conf_data[\"raw_train_json\"]\n",
        "\n",
        "    push_to_hub                    = conf_data[\"push_to_hub\"]\n",
        "    tokenizer_repo_name            = conf_data[\"tokenizer_repo_name\"]\n",
        "    tokenizer_output_dir           = conf_data[\"tokenizer_output_dir\"]\n",
        "    run_name                       = conf_data[\"push_to_hub\"]\n",
        "\n",
        "    # Collab only vars:\n",
        "    drive_mounted                  = conf_data[\"drive_mounted\"]\n",
        "    drive_mounted_path             = conf_data[\"drive_mounted_path\"]\n",
        "    train_data_archived            = conf_data[\"train_data_archived\"]\n",
        "    raw_train_archive              = conf_data[\"raw_train_archive\"]\n",
        "    user_email                     = conf_data[\"user_email\"]\n",
        "    user_name                      = conf_data[\"user_name\"]"
      ],
      "metadata": {
        "id": "zGhP0O2l971d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MOUNTING DRIVE\n",
        "if drive_mounted:\n",
        "    drive.mount(drive_mounted_path)"
      ],
      "metadata": {
        "id": "8xvHPjbL1K08"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PARSING CONFIG\n",
        "if config_file is not None:\n",
        "    parse_config(config_file)"
      ],
      "metadata": {
        "id": "M8mqgmLo-b6j"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURING GIT CREDENTIALS\n",
        "if push_to_hub:\n",
        "    !git config --global user.email \"{user_email}\"\n",
        "    !git config --global user.name \"{user_name}\"\n",
        "\n",
        "# To set Hugging Face token (for writing access) create HF_TOKEN secret in Google Collab or use notebook_login()"
      ],
      "metadata": {
        "id": "FvHd-jgLcUnR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURING GIT DIRECTORIES\n",
        "if push_to_hub:\n",
        "    repo = Repository(tokenizer_output_dir, clone_from=tokenizer_repo_name)\n",
        "    repo.git_checkout(run_name, create_branch_ok=True)"
      ],
      "metadata": {
        "id": "xS3pN5891Yq5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNPACK DATASET\n",
        "if train_data_archived:\n",
        "    if raw_train_archive.endswith(\".gz\"):\n",
        "        !gzip -dkv \"{raw_train_archive}\"\n",
        "    if raw_train_archive.endswith(\".zip\"):\n",
        "        !unzip \"{raw_train_archive}\""
      ],
      "metadata": {
        "id": "aNsuHVa51IjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATASET\n",
        "tokenizer_dataset = load_dataset(\"json\", data_files=raw_train_json, field=\"data\")\n",
        "print(tokenizer_dataset)"
      ],
      "metadata": {
        "id": "WAkN_SMaUxoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING BASE TOKENIZER\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model)"
      ],
      "metadata": {
        "id": "sSMHvqd9466r"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GET TRAINING CORPUS\n",
        "def get_training_corpus():\n",
        "    \"\"\"\n",
        "    Yields batch_size samples from training dataset.\n",
        "    \"\"\"\n",
        "    train_dataset = tokenizer_dataset[\"train\"]\n",
        "    for ind in range(0, len(train_dataset), batch_size):\n",
        "        samples = train_dataset[ind : ind + batch_size]\n",
        "        yield samples[\"content\"]"
      ],
      "metadata": {
        "id": "e3z2d636VGks"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING TOKENIZER\n",
        "training_corpus = get_training_corpus()\n",
        "tokenizer = base_tokenizer.train_new_from_iterator(training_corpus, vocab_size)\n",
        "print(\"Tokenizer Vocab Size: \", len(tokenizer))"
      ],
      "metadata": {
        "id": "zmI-RfNAWHI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e6f94c3-1fe6-4e65-f6a7-e16208cff368"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer Vocab Size:  30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING TOKENIZER\n",
        "tokenizer.save_pretrained(tokenizer_output_dir)\n",
        "if push_to_hub:\n",
        "    repo.push_to_hub(\n",
        "        commit_message=f\"experimental commit\", blocking=False\n",
        "    )"
      ],
      "metadata": {
        "id": "tkbWs-ipA3qM"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}