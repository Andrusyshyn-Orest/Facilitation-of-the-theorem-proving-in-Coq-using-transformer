{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "4H5T74YvTGQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import notebook_login, Repository\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "qfIvzG7b0u37"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMS AND GLOBAL VARS\n",
        "vocab_size                     = 30000                             # vocabulary size of the tokenizer\n",
        "base_model                     = \"gpt2\"                            # base model Hugging Face name\n",
        "batch_size                     = 20\n",
        "\n",
        "drive_mounted                  = True\n",
        "drive_mounted_path             = \"/content/gdrive/\"                # only needed if drive_mounted == True\n",
        "train_data_archived            = True                              # set to True if you need to extract the data from archive\n",
        "raw_train_archive              = drive_mounted_path + \"My Drive/UCU/diploma/datasets/dataset_train.zip\"    # only needed if data_archived == True\n",
        "raw_train_json                 = \"./dataset_train.json\"            # path to the train dataset (after extracting it will be in the current working directory)\n",
        "\n",
        "push_to_hub                    = False                             # set to True if you need to commit changes\n",
        "user_email                     = \"orest.andrusyshyn@ucu.edu.ua\"    # only needed if push_to_hub == True\n",
        "user_name                      = \"Orest Andrusyshyn\"               # only needed if push_to_hub == True\n",
        "tokenizer_repo_name            = \"Andrusyshyn/gpt2-coq-tokenizer\"  # only needed if push_to_hub == True\n",
        "tokenizer_output_dir           = \"gpt2-coq-tokenizer_local\"        # local dir to save the tokenizer\n",
        "run_name                       = \"experimental\"                    # branch name (only needed if push_to_hub == True)"
      ],
      "metadata": {
        "id": "RCwNJhLe0MdL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MOUNTING DRIVE\n",
        "if drive_mounted:\n",
        "    drive.mount(drive_mounted_path)"
      ],
      "metadata": {
        "id": "8xvHPjbL1K08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURING GIT CREDENTIALS\n",
        "if push_to_hub:\n",
        "    !git config --global user.email \"{user_email}\"\n",
        "    !git config --global user.name \"{user_name}\"\n",
        "\n",
        "# To set Hugging Face token (for writing access) create HF_TOKEN secret in Google Collab or use notebook_login()"
      ],
      "metadata": {
        "id": "FvHd-jgLcUnR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURING GIT DIRECTORIES\n",
        "if push_to_hub:\n",
        "    repo = Repository(tokenizer_output_dir, clone_from=tokenizer_repo_name)\n",
        "    repo.git_checkout(run_name, create_branch_ok=True)"
      ],
      "metadata": {
        "id": "xS3pN5891Yq5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNPACK DATASET\n",
        "if train_data_archived:\n",
        "    if raw_train_archive.endswith(\".gz\"):\n",
        "        !gzip -dkv \"{raw_train_archive}\"\n",
        "    if raw_train_archive.endswith(\".zip\"):\n",
        "        !unzip \"{raw_train_archive}\""
      ],
      "metadata": {
        "id": "aNsuHVa51IjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATASET\n",
        "tokenizer_dataset = load_dataset(\"json\", data_files=raw_train_json, field=\"data\")\n",
        "print(tokenizer_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAkN_SMaUxoY",
        "outputId": "b27ef24c-df1f-46a9-a44e-d989017e38ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['filepath', 'content'],\n",
            "        num_rows: 4972\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING BASE TOKENIZER\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model)"
      ],
      "metadata": {
        "id": "sSMHvqd9466r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GET TRAINING CORPUS\n",
        "def get_training_corpus():\n",
        "    \"\"\"\n",
        "    Yields batch_size samples from training dataset.\n",
        "    \"\"\"\n",
        "    train_dataset = tokenizer_dataset[\"train\"]\n",
        "    for ind in range(0, len(train_dataset), batch_size):\n",
        "        samples = train_dataset[ind : ind + batch_size]\n",
        "        yield samples[\"content\"]"
      ],
      "metadata": {
        "id": "e3z2d636VGks"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING TOKENIZER\n",
        "training_corpus = get_training_corpus()\n",
        "tokenizer = base_tokenizer.train_new_from_iterator(training_corpus, vocab_size)\n",
        "print(\"Tokenizer Vocab Size: \", len(tokenizer))"
      ],
      "metadata": {
        "id": "zmI-RfNAWHI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58a379b-75a1-436d-9ea2-f9b6721ca320"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer Vocab Size:  30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING TOKENIZER\n",
        "tokenizer.save_pretrained(tokenizer_output_dir)\n",
        "if push_to_hub:\n",
        "    repo.push_to_hub(\n",
        "        commit_message=f\"experimental commit\", blocking=False\n",
        "    )"
      ],
      "metadata": {
        "id": "tkbWs-ipA3qM"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}