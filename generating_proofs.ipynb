{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAstmePep_vQ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, pipeline, TextGenerationPipeline\n",
        "\n",
        "import json\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import runtime"
      ],
      "metadata": {
        "id": "lPEL1iqyqKkM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMS AND GLOBAL VARS\n",
        "do_generate_proofs             = True     # set to True to generate proofs\n",
        "sequence_length                = 1024     # input sequence length of the model\n",
        "max_new_tokens                 = 256      # only needed if do_generate_proofs == True\n",
        "batch_size                     = 2        # only needed if do_generate_proofs == True\n",
        "proofs_per_theorem             = 50       # only needed if do_generate_proofs == True\n",
        "temperature                    = 1        # only needed if do_generate_proofs == True\n",
        "do_sample                      = True     # only needed if do_generate_proofs == True\n",
        "top_p                          = 0.95     # only needed if do_generate_proofs == True\n",
        "\n",
        "model_repo_name                = \"Andrusyshyn/gpt2-pretrained-for-coq-pt-custom-train\"\n",
        "model_commit_hash              = \"32c2695d0f5f0b6117529f2eaa7f240b95cc42eb\"\n",
        "\n",
        "drive_mounted                  = False\n",
        "drive_mounted_path             = \"/content/gdrive/\"    # only needed if drive_mounted == True\n",
        "theorems_input_file            = \"./test_theorems_comp.json\"        # theorem dataset\n",
        "theorems_output_file           = \"./generated_comp_n06_k50.json\"    # output file for generated proofs\n",
        "\n",
        "config_file                    = None\n",
        "\n",
        "do_test_loss                   = False    # set ot True to calculate model loss on the test dataset\n",
        "test_batch_size                = 4        # only needed if do_test_loss == True\n",
        "test_data_archived             = True     # set to True if you need to extract the data from archive\n",
        "raw_test_archive               = \"./dataset_test.zip\"     # only needed if data_archived == True\n",
        "raw_test_json                  = \"./dataset_test.json\"    # path to the test dataset (after extracting it will be in the current working directory)\n",
        "\n",
        "torch_seed                     = 77"
      ],
      "metadata": {
        "id": "MDt9V_0_r0XZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a05ed57-9181-4ccb-cc24-8527fe076849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ca2a04e27d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_config(config_file: str):\n",
        "    \"\"\"\n",
        "    Parses config_file and sets global variables.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config_file : str\n",
        "        path to config file.\n",
        "    \"\"\"\n",
        "    global do_generate_proofs, sequence_length, max_new_tokens, batch_size, proofs_per_theorem,\\\n",
        "           temperature, do_sample, top_p, model_repo_name, model_commit_hash, theorems_input_file,\\\n",
        "           theorems_output_file, do_test_loss, test_batch_size, raw_test_json, use_gpu, torch_seed,\\\n",
        "           drive_mounted, drive_mounted_path, test_data_archived, raw_test_archive\n",
        "\n",
        "    with open(config_file, mode='r') as conf_file:\n",
        "        conf_data = json.load(conf_file)\n",
        "\n",
        "    do_generate_proofs             = conf_data[\"do_generate_proofs\"]\n",
        "    sequence_length                = conf_data[\"sequence_length\"]\n",
        "    max_new_tokens                 = conf_data[\"max_new_tokens\"]\n",
        "    batch_size                     = conf_data[\"batch_size\"]\n",
        "    proofs_per_theorem             = conf_data[\"proofs_per_theorem\"]\n",
        "    temperature                    = conf_data[\"temperature\"]\n",
        "    do_sample                      = conf_data[\"do_sample\"]\n",
        "    top_p                          = conf_data[\"top_p\"]\n",
        "\n",
        "    model_repo_name                = conf_data[\"model_repo_name\"]\n",
        "    model_commit_hash              = conf_data[\"model_commit_hash\"]\n",
        "\n",
        "    theorems_input_file            = conf_data[\"theorems_input_file\"]\n",
        "    theorems_output_file           = conf_data[\"theorems_output_file\"]\n",
        "\n",
        "    do_test_loss                   = conf_data[\"do_test_loss\"]\n",
        "    test_batch_size                = conf_data[\"test_batch_size\"]\n",
        "    raw_test_json                  = conf_data[\"raw_test_json\"]\n",
        "\n",
        "    torch_seed                     = conf_data[\"torch_seed\"]\n",
        "\n",
        "    # Collab-only global vars:\n",
        "    drive_mounted                  = conf_data[\"drive_mounted\"]\n",
        "    drive_mounted_path             = conf_data[\"drive_mounted_path\"]\n",
        "    test_data_archived             = conf_data[\"test_data_archived\"]\n",
        "    raw_test_archive               = conf_data[\"raw_test_archive\"]"
      ],
      "metadata": {
        "id": "Bv-m4f-0njzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(inputs: torch.Tensor, logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculates mean CrossEntropyLoss across samples in the batch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    inputs : torch.Tensor\n",
        "        tensor of input sequences. Dimensions: batch_size X context_length.\n",
        "    logits: torch.\n",
        "        logits outputted by model. Dimensions: batch_size X context_length X vocab_size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        mean CrossEntropyLoss loss across samples in the batch.\n",
        "    \"\"\"\n",
        "    # inputs [batch_size X cl]\n",
        "    # logits [batch_size X cl X vocab_size]\n",
        "    # Our labels start from second sequence token because first one does not have preceding token.\n",
        "    # We drop last logit because last sequence token does not have subsequent token, so no label to compare\n",
        "    shifted_labels = inputs[..., 1:].contiguous()\n",
        "    shifted_logits = logits[..., :-1, :].contiguous()\n",
        "    loss_func = CrossEntropyLoss(reduction='none')\n",
        "    # loss [batch_size * (cl-1)] = loss_fct([batch_size * (cl-1) X vocab_size], [batch_size * (cl-1)])\n",
        "    loss = loss_func(shifted_logits.view(-1, shifted_logits.size(-1)), shifted_labels.view(-1))\n",
        "    # loss_per_sequence [batch_size]\n",
        "    loss_per_sequence = loss.view(shifted_logits.size(0), shifted_logits.size(1)).mean(axis=1)\n",
        "    return loss_per_sequence.mean()"
      ],
      "metadata": {
        "id": "cvV9Heee4HYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loss(p_model: GPT2LMHeadModel, p_test_dataloader: DataLoader, p_device: torch.device) -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Calculates test loss and perplexity of the p_model on the test dataset from p_test_dataloader.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    p_model : GPT2LMHeadModel\n",
        "        model to test.\n",
        "    p_test_dataloader : DataLoader\n",
        "        Dataloader with test data.\n",
        "    p_device : torch.device\n",
        "        cpu or cuda.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[float, float]\n",
        "        test loss, test perplexity\n",
        "    \"\"\"\n",
        "    print(type(p_model), type(p_test_dataloader), type(p_device))\n",
        "    if use_gpu and torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "    p_model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in p_test_dataloader:\n",
        "            with torch.no_grad():\n",
        "                input_ids = batch[\"input_ids\"].to(p_device)\n",
        "                logits = p_model(input_ids).logits\n",
        "                loss = loss_function(input_ids, logits)\n",
        "                losses.append(loss.item())\n",
        "    loss = torch.mean(torch.Tensor(losses))\n",
        "    try:\n",
        "        perplexity = torch.exp(loss)\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "\n",
        "    if use_gpu and torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    return loss.item(), perplexity.item()"
      ],
      "metadata": {
        "id": "R7pWkTT55dB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenized_dataset(p_raw_dataset: Dataset, p_context_length: int, p_tokenizer: AutoTokenizer) -> Dataset:\n",
        "    \"\"\"\n",
        "    Tokenizes raw dataset p_raw_dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    p_raw_dataset : Dataset\n",
        "        raw dataset ot tokenize\n",
        "    p_context_length : int\n",
        "        context length\n",
        "    p_tokenizer : AutoTokenizer\n",
        "        tokenizer\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dataset\n",
        "        tokenized dataset, each entry is the input sequence of the\n",
        "        context_length length.\n",
        "    \"\"\"\n",
        "    concatenated_tokenized_samples = []\n",
        "    for sample in p_raw_dataset:\n",
        "        tokenized_sample = p_tokenizer(sample[\"content\"], truncation=False)[\"input_ids\"]\n",
        "        concatenated_tokenized_samples.extend(tokenized_sample + [p_tokenizer.eos_token_id])\n",
        "\n",
        "    tokenized_dataset_list = []\n",
        "    for i in range(0, len(concatenated_tokenized_samples), p_context_length):\n",
        "        input_ids = concatenated_tokenized_samples[i : i + p_context_length]\n",
        "        if len(input_ids) == p_context_length:\n",
        "            tokenized_dataset_list.append(torch.tensor(input_ids))\n",
        "\n",
        "    return Dataset.from_dict({\"input_ids\": tokenized_dataset_list})"
      ],
      "metadata": {
        "id": "orRR7rXZ0GWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_theorem_statement(theorem_with_proof: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts theorem statement from theorem_with_proof.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    theorem_with_proof : str\n",
        "        theorem with proof.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        theorem statement.\n",
        "    \"\"\"\n",
        "    pos = theorem_with_proof.find(\"\\nProof.\")\n",
        "    if pos != -1:\n",
        "        return theorem_with_proof[:pos]\n",
        "\n",
        "    print(\"THEOREM PROOF DOES NOT START WITH 'Proof.'\")\n",
        "    return theorem_with_proof"
      ],
      "metadata": {
        "id": "77PpCyp8s7Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_on_Qed(generated_proof: str) -> tuple[str, bool]:\n",
        "    \"\"\"\n",
        "    Truncate generated_proof on the first occurance of\n",
        "    \"Qed.\", \"Defined.\" or \"Save\".\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    generated_proof : str\n",
        "        generated proof\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[str, bool]\n",
        "        First element is the trucated proof. Second element is status.\n",
        "        Status is False if no finalization command were found. In such\n",
        "        case, first element is just whole generated_proof.\n",
        "    \"\"\"\n",
        "    qed_stop = \"Qed.\"\n",
        "    defined_stop = \"Defined.\"\n",
        "    save_stop = \"Save\"\n",
        "\n",
        "    pos_qed = generated_proof.find(qed_stop)\n",
        "    pos_defined = generated_proof.find(defined_stop)\n",
        "    pos_save = generated_proof.find(save_stop)\n",
        "\n",
        "    poses_stops = []\n",
        "    if (pos_qed != -1):\n",
        "        poses_stops.append(pos_qed)\n",
        "    if (pos_defined != -1):\n",
        "        poses_stops.append(pos_defined)\n",
        "    if (pos_save != -1):\n",
        "        poses_stops.append(pos_save)\n",
        "    if (poses_stops == []):\n",
        "        return (generated_proof, False)\n",
        "\n",
        "    return (generated_proof[:min(poses_stops)], True)"
      ],
      "metadata": {
        "id": "kv8VXRuNwMDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_proofs(input_file: str, output_file: str, p_pipe: TextGenerationPipeline, num_proofs: int):\n",
        "    \"\"\"\n",
        "    Iterates over each theorem entry in the input_file. Generates num_proofs per theorem.\n",
        "    Uses generation hyperparameters from global config:\n",
        "    batch_size, max_new_tokens, do_sample, top_p, temperature, sequence_length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_file : str\n",
        "        path to the file with theorems (theorem dataset).\n",
        "    output_file : str\n",
        "        output path of the tested theorems.\n",
        "    p_pipe : TextGenerationPipeline\n",
        "        text-generation pipeline\n",
        "    num_proofs : int\n",
        "        number of generated proofs per theorem.\n",
        "    \"\"\"\n",
        "    cuda_available = torch.cuda.is_available()\n",
        "    new_json_data = None\n",
        "    with open(input_file, mode='r') as json_input:\n",
        "        new_json_data = json.load(json_input)\n",
        "\n",
        "    theorems_processed = 0\n",
        "    proofs_with_end = 0\n",
        "    for project in new_json_data[\"projects\"].keys():\n",
        "        for i in range(0, len(new_json_data[\"projects\"][project]), batch_size):\n",
        "            theorems = new_json_data[\"projects\"][project][i:i+batch_size]\n",
        "            input_sequences = []\n",
        "            theorem_declarations = []\n",
        "            for theorem in theorems:\n",
        "                theorem_declaration = extract_theorem_statement(theorem[\"proof\"])\n",
        "                theorem_declarations.append(theorem_declaration)\n",
        "                input_sequence = theorem[\"context\"] + theorem_declaration\n",
        "                input_sequences.append(input_sequence)\n",
        "\n",
        "            generated_texts = p_pipe(input_sequences, num_return_sequences=num_proofs,\n",
        "                                     max_new_tokens=max_new_tokens,\n",
        "                                     return_full_text=False,\n",
        "                                     do_sample=True, top_p=top_p, temperature=temperature)\n",
        "            if cuda_available:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            ind = 0\n",
        "            for generated_text in generated_texts:\n",
        "                generated_proofs = []\n",
        "                for proof in generated_text:\n",
        "                    proof_with_no_context = theorem_declarations[ind] + proof['generated_text']\n",
        "                    truncated_proof, found_end = truncate_on_Qed(proof_with_no_context)\n",
        "                    if found_end:\n",
        "                        proofs_with_end += 1\n",
        "                    generated_proofs.append(truncated_proof + theorems[ind][\"end_command\"])\n",
        "                new_json_data[\"projects\"][project][ind+i][\"generated_proofs\"] = generated_proofs\n",
        "                theorems_processed += 1\n",
        "                ind += 1\n",
        "\n",
        "            if theorems_processed % 10 == 0:\n",
        "                print(theorems_processed)\n",
        "                with open(output_file, mode='w') as json_output:\n",
        "                    json.dump(new_json_data, json_output, indent=4)\n",
        "\n",
        "    new_json_data[\"hyperparams\"] = {\n",
        "        \"sequence_length\": sequence_length,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"proofs_per_theorem\": proofs_per_theorem,\n",
        "        \"temperature\": temperature,\n",
        "        \"do_sample\": do_sample,\n",
        "        \"top_p\": top_p,\n",
        "        \"model_repo_name\": model_repo_name,\n",
        "        \"model_commit_hash\": model_commit_hash,\n",
        "        \"torch_seed\": torch_seed\n",
        "    }\n",
        "    with open(output_file, mode='w') as json_output:\n",
        "        json.dump(new_json_data, json_output, indent=4)\n",
        "    print(\"Theorems Processed: \", theorems_processed)\n",
        "    print(\"Proofs with end:    \", proofs_with_end)"
      ],
      "metadata": {
        "id": "KRM8oho9rLrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if drive_mounted:\n",
        "    drive.mount(drive_mounted_path)"
      ],
      "metadata": {
        "id": "zpPZ3FCsuw55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PARSING CONFIG FILE\n",
        "if config_file is not None:\n",
        "    parse_config(config_file)\n",
        "torch.manual_seed(torch_seed)"
      ],
      "metadata": {
        "id": "vDW6qjaRrppX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNPACK DATASETS\n",
        "if do_test_loss:\n",
        "    if test_data_archived:\n",
        "        if raw_test_archive.endswith(\".gz\"):\n",
        "            !gzip -dkv \"{raw_test_archive}\"\n",
        "        if raw_test_archive.endswith(\".zip\"):\n",
        "            !unzip \"{raw_test_archive}\""
      ],
      "metadata": {
        "id": "bz5s04S30Qn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD TEST DATASET\n",
        "if do_test_loss:\n",
        "    raw_test_dataset = load_dataset(\"json\", data_files=raw_test_json, field=\"data\")\n",
        "    print(raw_test_dataset)"
      ],
      "metadata": {
        "id": "JhREingP0plT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD MODEL AND TOKENIZER\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "coq_tokenizer = AutoTokenizer.from_pretrained(model_repo_name, revision=model_commit_hash)\n",
        "coq_model = GPT2LMHeadModel.from_pretrained(model_repo_name, revision=model_commit_hash).to(device)\n",
        "print(f\"Tokenizer vocab size:                              {len(coq_tokenizer)}\")\n",
        "print(f\"Model size:                                        {coq_model.num_parameters()}\")\n",
        "print(f\"Model size (only trainable params):                {coq_model.num_parameters(only_trainable=True)}\")\n",
        "print(f\"Model size (only trainable non-embeddings params): {coq_model.num_parameters(only_trainable=True, exclude_embeddings=True)}\")\n",
        "pipe = pipeline(\"text-generation\", model=coq_model, tokenizer=coq_tokenizer, batch_size=batch_size, device=0 if torch.cuda.is_available() else -1)\n",
        "pipe.tokenizer.pad_token_id = coq_model.config.eos_token_id\n",
        "pipe.tokenizer.padding_side = 'left'"
      ],
      "metadata": {
        "id": "hMJMbFUNqxOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if do_test_loss:\n",
        "    # TOKENIZE RAW DATASET\n",
        "    test_dataset = get_tokenized_dataset(raw_test_dataset[\"train\"], sequence_length, coq_tokenizer)\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    # CREATE DATALOADER\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
        "    print(test_dataset)\n",
        "    print(\"len(test_dataloader): \", len(test_dataloader))"
      ],
      "metadata": {
        "id": "ELdcEjMq3DVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATE TEST LOSS\n",
        "if do_test_loss:\n",
        "    _loss,_perp = test_loss(coq_model, test_dataloader, device)\n",
        "    print(\"Test Loss:       \", _loss)\n",
        "    print(\"Test Perplexity: \", _perp)"
      ],
      "metadata": {
        "id": "9YleZJwx3Pzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROOF GENERATION\n",
        "if do_generate_proofs:\n",
        "    time_start = time.perf_counter()\n",
        "    generate_proofs(theorems_input_file, theorems_output_file, pipe, proofs_per_theorem)\n",
        "    time_end = time.perf_counter()\n",
        "    print(f\"Total time: {time_end - time_start} seconds\")"
      ],
      "metadata": {
        "id": "RPSx0QqNqdkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runtime.unassign()"
      ],
      "metadata": {
        "id": "vInksapFK6lp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}