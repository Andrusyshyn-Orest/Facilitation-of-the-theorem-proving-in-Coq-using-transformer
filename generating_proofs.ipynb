{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAstmePep_vQ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, pipeline, AutoModel, AutoConfig\n",
        "\n",
        "import json\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import runtime"
      ],
      "metadata": {
        "id": "lPEL1iqyqKkM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMS AND GLOBAL VARS\n",
        "do_generate_proofs             = True     # set to True to generate proofs\n",
        "sequence_length                = 1024     # input sequence length of the model\n",
        "max_new_tokens                 = 256      # only needed if do_generate_proofs == True\n",
        "batch_size                     = 2        # only needed if do_generate_proofs == True\n",
        "proofs_per_theorem             = 50       # only needed if do_generate_proofs == True\n",
        "temperature                    = 1        # only needed if do_generate_proofs == True\n",
        "do_sample                      = True     # only needed if do_generate_proofs == True\n",
        "top_p                          = 0.95     # only needed if do_generate_proofs == True\n",
        "\n",
        "model_repo_name                = \"Andrusyshyn/gpt2-pretrained-for-coq-pt-custom-train\"\n",
        "model_commit_hash              = \"32c2695d0f5f0b6117529f2eaa7f240b95cc42eb\"\n",
        "\n",
        "drive_mounted                  = True\n",
        "drive_mounted_path             = \"/content/gdrive/\"    # only needed if drive_mounted == True\n",
        "theorems_input_file            = drive_mounted_path + \"My Drive/UCU/diploma/theorems/test_theorems_comp.json\"\n",
        "theorems_output_file           = drive_mounted_path + \"My Drive/UCU/diploma/theorems/generated_comp_n06_k50.json\"\n",
        "\n",
        "do_test_loss                   = False    # set ot True to calculate model loss on the test dataset\n",
        "test_batch_size                = 4        # only needed if do_test_loss == True\n",
        "test_data_archived             = True     # set to True if you need to extract the data from archive\n",
        "raw_test_archive               = drive_mounted_path + \"My Drive/UCU/diploma/datasets/dataset_test.zip\"    # only needed if data_archived == True\n",
        "raw_test_json                  = \"./dataset_test.json\"    # path to the test dataset (after extracting it will be in the current working directory)\n",
        "\n",
        "torch_seed                     = 77\n",
        "torch.manual_seed(torch_seed)"
      ],
      "metadata": {
        "id": "MDt9V_0_r0XZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a05ed57-9181-4ccb-cc24-8527fe076849"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ca2a04e27d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUSTOM LOSS FUNCTION\n",
        "def loss_function(inputs, logits):\n",
        "    shifted_labels = inputs[..., 1:].contiguous()\n",
        "    shifted_logits = logits[..., :-1, :].contiguous()\n",
        "    loss_func = CrossEntropyLoss(reduction='none')\n",
        "    loss = loss_func(shifted_logits.view(-1, shifted_logits.size(-1)), shifted_labels.view(-1))\n",
        "    loss_per_sequence = loss.view(shifted_logits.size(0), shifted_logits.size(1)).mean(axis=1)\n",
        "    return loss_per_sequence.mean()"
      ],
      "metadata": {
        "id": "cvV9Heee4HYJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOSS EVALUATION FUNCTION\n",
        "def test_loss(p_model, p_test_dataloader, p_device):\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "    p_model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(p_test_dataloader):\n",
        "            with torch.no_grad():\n",
        "                input_ids = batch[\"input_ids\"].to(p_device)\n",
        "                logits = p_model(input_ids).logits\n",
        "                loss = loss_function(input_ids, logits)\n",
        "                losses.append(loss.item())\n",
        "    loss = torch.mean(torch.Tensor(losses))\n",
        "    try:\n",
        "        perplexity = torch.exp(loss)\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    return loss.item(), perplexity.item()"
      ],
      "metadata": {
        "id": "R7pWkTT55dB7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZE RAW DATASET\n",
        "def get_tokenized_dataset(p_raw_dataset, p_context_length, p_tokenizer):\n",
        "    concatenated_tokenized_samples = []\n",
        "    for sample in p_raw_dataset:\n",
        "        tokenized_sample = p_tokenizer(sample[\"content\"], truncation=False)[\"input_ids\"]\n",
        "        concatenated_tokenized_samples.extend(tokenized_sample + [p_tokenizer.eos_token_id])\n",
        "\n",
        "    tokenized_dataset_list = []\n",
        "    for i in range(0, len(concatenated_tokenized_samples), p_context_length):\n",
        "        input_ids = concatenated_tokenized_samples[i : i + p_context_length]\n",
        "        if len(input_ids) == p_context_length:\n",
        "            tokenized_dataset_list.append(torch.tensor(input_ids))\n",
        "\n",
        "    return Dataset.from_dict({\"input_ids\": tokenized_dataset_list})"
      ],
      "metadata": {
        "id": "orRR7rXZ0GWT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRACT THEOREM STATEMENT FROM WHOLE PROOF\n",
        "def extract_theorem_statement(theorem_with_proof):\n",
        "    pos = theorem_with_proof.find(\"\\nProof.\")\n",
        "    if pos != -1:\n",
        "        return theorem_with_proof[:pos]\n",
        "\n",
        "    print(\"THEOREM PROOF DOES NOT START WITH 'Proof.'\")\n",
        "    return theorem_with_proof"
      ],
      "metadata": {
        "id": "77PpCyp8s7Nt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRUNCATE PROOF TILL STOP WORD\n",
        "def truncate_on_Qed(generated_proof: str):\n",
        "    qed_stop = \"Qed.\"\n",
        "    defined_stop = \"Defined.\"\n",
        "    save_stop = \"Save\"\n",
        "\n",
        "    pos_qed = generated_proof.find(qed_stop)\n",
        "    pos_defined = generated_proof.find(defined_stop)\n",
        "    pos_save = generated_proof.find(save_stop)\n",
        "\n",
        "    poses_stops = []\n",
        "    if (pos_qed != -1):\n",
        "        poses_stops.append(pos_qed)\n",
        "    if (pos_defined != -1):\n",
        "        poses_stops.append(pos_defined)\n",
        "    if (pos_save != -1):\n",
        "        poses_stops.append(pos_save)\n",
        "    if (poses_stops == []):\n",
        "        return (generated_proof, False)\n",
        "\n",
        "    return (generated_proof[:min(poses_stops)], True)"
      ],
      "metadata": {
        "id": "kv8VXRuNwMDJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROOF GENERATION\n",
        "def generate_proofs(input_file, output_file, p_pipe, num_proofs):\n",
        "    cuda_available = torch.cuda.is_available()\n",
        "    new_json_data = None\n",
        "    with open(input_file, mode='r') as json_input:\n",
        "        new_json_data = json.load(json_input)\n",
        "\n",
        "    theorems_processed = 0\n",
        "    proofs_with_end = 0\n",
        "    for project in new_json_data[\"projects\"].keys():\n",
        "        for i in range(0, len(new_json_data[\"projects\"][project]), batch_size):\n",
        "            theorems = new_json_data[\"projects\"][project][i:i+batch_size]\n",
        "            input_sequences = []\n",
        "            theorem_declarations = []\n",
        "            for theorem in theorems:\n",
        "                theorem_declaration = extract_theorem_statement(theorem[\"proof\"])\n",
        "                theorem_declarations.append(theorem_declaration)\n",
        "                input_sequence = theorem[\"context\"] + theorem_declaration\n",
        "                input_sequences.append(input_sequence)\n",
        "\n",
        "            generated_texts = p_pipe(input_sequences, num_return_sequences=num_proofs,\n",
        "                                     max_new_tokens=max_new_tokens,\n",
        "                                     return_full_text=False,\n",
        "                                     do_sample=True, top_p=top_p, temperature=temperature)\n",
        "            if cuda_available:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            ind = 0\n",
        "            for generated_text in generated_texts:\n",
        "                generated_proofs = []\n",
        "                for proof in generated_text:\n",
        "                    proof_with_no_context = theorem_declarations[ind] + proof['generated_text']\n",
        "                    truncated_proof, found_end = truncate_on_Qed(proof_with_no_context)\n",
        "                    if found_end:\n",
        "                        proofs_with_end += 1\n",
        "                    generated_proofs.append(truncated_proof + theorems[ind][\"end_command\"])\n",
        "                new_json_data[\"projects\"][project][ind+i][\"generated_proofs\"] = generated_proofs\n",
        "                theorems_processed += 1\n",
        "                ind += 1\n",
        "\n",
        "            if theorems_processed % 10 == 0:\n",
        "                print(theorems_processed)\n",
        "                with open(output_file, mode='w') as json_output:\n",
        "                    json.dump(new_json_data, json_output, indent=4)\n",
        "\n",
        "    new_json_data[\"hyperparams\"] = {\n",
        "        \"sequence_length\": sequence_length,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"proofs_per_theorem\": proofs_per_theorem,\n",
        "        \"temperature\": temperature,\n",
        "        \"do_sample\": do_sample,\n",
        "        \"top_p\": top_p,\n",
        "        \"model_repo_name\": model_repo_name,\n",
        "        \"model_commit_hash\": model_commit_hash,\n",
        "        \"torch_seed\": torch_seed\n",
        "    }\n",
        "    with open(output_file, mode='w') as json_output:\n",
        "        json.dump(new_json_data, json_output, indent=4)\n",
        "    print(\"Theorems Processed: \", theorems_processed)\n",
        "    print(\"Proofs with end:    \", proofs_with_end)"
      ],
      "metadata": {
        "id": "KRM8oho9rLrN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if drive_mounted:\n",
        "    drive.mount(drive_mounted_path)"
      ],
      "metadata": {
        "id": "zpPZ3FCsuw55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNPACK DATASETS\n",
        "if do_test_loss:\n",
        "    if test_data_archived:\n",
        "        if raw_test_archive.endswith(\".gz\"):\n",
        "            !gzip -dkv \"{raw_test_archive}\"\n",
        "        if raw_test_archive.endswith(\".zip\"):\n",
        "            !unzip \"{raw_test_archive}\""
      ],
      "metadata": {
        "id": "bz5s04S30Qn3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD TEST DATASET\n",
        "if do_test_loss:\n",
        "    raw_test_dataset = load_dataset(\"json\", data_files=raw_test_json, field=\"data\")\n",
        "    print(raw_test_dataset)"
      ],
      "metadata": {
        "id": "JhREingP0plT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD MODEL AND TOKENIZER\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "coq_tokenizer = AutoTokenizer.from_pretrained(model_repo_name, revision=model_commit_hash)\n",
        "coq_model = GPT2LMHeadModel.from_pretrained(model_repo_name, revision=model_commit_hash).to(device)\n",
        "print(f\"Tokenizer vocab size:                              {len(coq_tokenizer)}\")\n",
        "print(f\"Model size:                                        {coq_model.num_parameters()}\")\n",
        "print(f\"Model size (only trainable params):                {coq_model.num_parameters(only_trainable=True)}\")\n",
        "print(f\"Model size (only trainable non-embeddings params): {coq_model.num_parameters(only_trainable=True, exclude_embeddings=True)}\")\n",
        "pipe = pipeline(\"text-generation\", model=coq_model, tokenizer=coq_tokenizer, batch_size=batch_size, device=0 if torch.cuda.is_available() else -1)\n",
        "pipe.tokenizer.pad_token_id = coq_model.config.eos_token_id\n",
        "pipe.tokenizer.padding_side = 'left'"
      ],
      "metadata": {
        "id": "hMJMbFUNqxOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if do_test_loss:\n",
        "    # TOKENIZE RAW DATASET\n",
        "    test_dataset = get_tokenized_dataset(raw_test_dataset[\"train\"], sequence_length, coq_tokenizer)\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    # CREATE DATALOADER\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
        "    print(test_dataset)\n",
        "    print(\"len(test_dataloader): \", len(test_dataloader))"
      ],
      "metadata": {
        "id": "ELdcEjMq3DVY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATE TEST LOSS\n",
        "if do_test_loss:\n",
        "    _loss,_perp = test_loss(coq_model, test_dataloader, device)\n",
        "    print(\"Test Loss:       \", _loss)\n",
        "    print(\"Test Perplexity: \", _perp)"
      ],
      "metadata": {
        "id": "9YleZJwx3Pzc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROOF GENERATION\n",
        "if do_generate_proofs:\n",
        "    time_start = time.perf_counter()\n",
        "    generate_proofs(theorems_input_file, theorems_output_file, pipe, proofs_per_theorem)\n",
        "    time_end = time.perf_counter()\n",
        "    print(f\"Total time: {time_end - time_start} seconds\")"
      ],
      "metadata": {
        "id": "RPSx0QqNqdkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "vInksapFK6lp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}