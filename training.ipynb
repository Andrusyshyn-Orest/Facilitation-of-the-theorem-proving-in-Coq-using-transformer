{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1GRus8L7m1aK9446WXPlrEO82RCS44ncz","timestamp":1711661116264}],"collapsed_sections":["9vi6XSOUTtO8","_srRfnAxr_cG"],"gpuType":"T4","authorship_tag":"ABX9TyPiWlFa9rNd2Y0Izd42jQDH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"febf83b414de40bfacb03e44cc32017a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b058a30805df4dcd979274c0a937be9c","IPY_MODEL_2635229f427d4a3d9a3f5cc5d5a78a6e","IPY_MODEL_119ee85adc814147bcea0201454f62b2"],"layout":"IPY_MODEL_3b2750ebf3c544989fd99ed60ab976d4"}},"b058a30805df4dcd979274c0a937be9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05e94ea5af3149f48817ae96b79ca6ab","placeholder":"​","style":"IPY_MODEL_89b9daa9adba425fa41fb825dfc3b5ff","value":"100%"}},"2635229f427d4a3d9a3f5cc5d5a78a6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5497e12b059249e490755d11fdbef9e7","max":5661,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8123a5252c84e8ba1b6963a7fd154d1","value":5661}},"119ee85adc814147bcea0201454f62b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39802cced75449e49e8ac3289576d3c4","placeholder":"​","style":"IPY_MODEL_a9e87d8c653f4d3496eb09ca49c3b1a0","value":" 5661/5661 [1:04:27&lt;00:00,  1.88it/s]"}},"3b2750ebf3c544989fd99ed60ab976d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05e94ea5af3149f48817ae96b79ca6ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89b9daa9adba425fa41fb825dfc3b5ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5497e12b059249e490755d11fdbef9e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8123a5252c84e8ba1b6963a7fd154d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39802cced75449e49e8ac3289576d3c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9e87d8c653f4d3496eb09ca49c3b1a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"958dfde2e2b245c89f4c9b133449806b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7eeb849972143adaf61dac857415e22","IPY_MODEL_92498eea1caa41809497ec442c9090e4","IPY_MODEL_df0c2020db7e439f8d167510cdcd36da"],"layout":"IPY_MODEL_1ebd954e05b24234b26b9287f1e71c8a"}},"b7eeb849972143adaf61dac857415e22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b6b8d7ca9db4a1aa7b5c11539ef9264","placeholder":"​","style":"IPY_MODEL_c89d8519c6094258ad357f79f53ae035","value":"100%"}},"92498eea1caa41809497ec442c9090e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_439e4a1ebfe44af5bd6236bfc8e4981e","max":5661,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48e167315bf447bba1f0c23b0fcc71b6","value":5661}},"df0c2020db7e439f8d167510cdcd36da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa3e209a4129447f920d421b9ac0ed22","placeholder":"​","style":"IPY_MODEL_02b5392e5dd4430b87a5198fc40b0c46","value":" 5661/5661 [1:07:44&lt;00:00,  1.98it/s]"}},"1ebd954e05b24234b26b9287f1e71c8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b6b8d7ca9db4a1aa7b5c11539ef9264":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c89d8519c6094258ad357f79f53ae035":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"439e4a1ebfe44af5bd6236bfc8e4981e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48e167315bf447bba1f0c23b0fcc71b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa3e209a4129447f920d421b9ac0ed22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02b5392e5dd4430b87a5198fc40b0c46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb9b4756bf804b0f9879f003fd24fbfc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ff9040a0494408aa5a98563953b07cd","IPY_MODEL_1748549ab76b45fc848858cfbf568797","IPY_MODEL_dcbbacafab1c46a38facbab1bb51d719"],"layout":"IPY_MODEL_57ae7579a5b9426fad84796940a6f133"}},"0ff9040a0494408aa5a98563953b07cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac3f6cf2ada2449c8c91ab8d5abaaff6","placeholder":"​","style":"IPY_MODEL_49de93d47afc41e18dce25d477925400","value":" 56%"}},"1748549ab76b45fc848858cfbf568797":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5fa25a4b7ad493080c080220295a1f6","max":5661,"min":0,"orientation":"horizontal","style":"IPY_MODEL_558de3458ac1471f8e7670a715573c3d","value":3143}},"dcbbacafab1c46a38facbab1bb51d719":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_faa022e4668f40f8b369f9e9b1cb61ca","placeholder":"​","style":"IPY_MODEL_fbb4fab7a4d64169957f3c50934478ad","value":" 3143/5661 [36:30&lt;20:49,  2.02it/s]"}},"57ae7579a5b9426fad84796940a6f133":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac3f6cf2ada2449c8c91ab8d5abaaff6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49de93d47afc41e18dce25d477925400":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5fa25a4b7ad493080c080220295a1f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"558de3458ac1471f8e7670a715573c3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"faa022e4668f40f8b369f9e9b1cb61ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbb4fab7a4d64169957f3c50934478ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TlOfuAT4e-xu"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!pip install accelerate\n","!apt install git-lfs"]},{"cell_type":"code","source":["import torch\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data.dataloader import DataLoader\n","from torch.optim import AdamW\n","from torch.utils.tensorboard import SummaryWriter\n","%load_ext tensorboard\n","\n","from huggingface_hub import notebook_login, Repository\n","from datasets import load_dataset, DatasetDict, Dataset\n","from accelerate import Accelerator\n","from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, pipeline, get_scheduler\n","from transformers.keras_callbacks import PushToHubCallback\n","\n","import re\n","import json\n","import os\n","from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","from google.colab import drive"],"metadata":{"id":"J6UDRGWQg4UD","executionInfo":{"status":"ok","timestamp":1711582429773,"user_tz":-120,"elapsed":7777,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### TRAINING\n"],"metadata":{"id":"iy7X6uix8Odn"}},{"cell_type":"markdown","source":["#### HYPER PARAMS, GLOBAL VARS, FUNCTIONS"],"metadata":{"id":"PbZLtjmyh0Tb"}},{"cell_type":"code","source":["# HYPER PARAMETERS\n","context_length                 = 1024\n","train_batch_size               = 4\n","eval_batch_size                = 4\n","weight_decay                   = 0.1\n","lr                             = 5e-4\n","num_warmup_steps               = 50\n","gradient_accumulation_steps    = 8\n","gradient_checkpointing         = False\n","eval_steps                     = 1000 // gradient_accumulation_steps\n","num_train_epochs               = 5\n","mixed_precision                = \"fp16\"\n","\n","#GLOBAL VARS\n","have_git_write_access          = True                              # set to True of you need to commit changes\n","user_email                     = \"orest.andrusyshyn@ucu.edu.ua\"    # only needed if above have_git_write_access == True\n","user_name                      = \"Orest Andrusyshyn\"               # only needed if above have_git_write_access == True\n","\n","drive_mounted                  = True\n","drive_mounted_path             = '/content/gdrive/'    # only needed if drive_mounted == True\n","save_json_logs                 = True                  # set to True to save training results into json file (drive_train_res_path)\n","drive_train_res_path           = drive_mounted_path + 'My Drive/UCU/diploma/progress_notes/training_results/second_training/theorems_cl_1024_voc_30k_train_res1.json'    # only needed if save_json_logs == True\n","save_tensorboard_logs          = True                  # set to True to save training results into tensorboard run (tensorboard_run_path)\n","tensorboard_run_path           = drive_mounted_path + 'My Drive/UCU/diploma/progress_notes/training_results/tensorboard_runs/theorems_cl_1024_voc_30k'                   # only needed if save_tensorboard_logs == True\n","\n","data_archived                  = True    # set to True if you need to unarchive the data\n","raw_train_archive              = \"./pretrain_dataset_train.json.gz\"         # only needed if data_archived == True\n","raw_valid_archive              = \"./pretrain_dataset_validation.json.gz\"    # only needed if data_archived == True\n","raw_train_json                 = \"./pretrain_dataset_train.json\"            # only needed if data_archived == False\n","raw_valid_json                 = \"./pretrain_dataset_validation.json\"       # only needed if data_archived == False\n","\n","tokenizer_repo_name            = \"Andrusyshyn/gpt2-coq-tokenizer\"\n","tokenizer_commit_hash          = \"687b6ed5c0eb5b4016323e7798c03af749fdeb31\"\n","\n","init_model                     = \"gpt2\"                                                   # base model Hugging Face name\n","model_repo_name                = \"Andrusyshyn/gpt2-pretrained-for-coq-pt-custom-train\"\n","model_output_dir               = \"./gpt2-pretrained-for-coq-pt-custom-train-local\"        # local dir to save the model\n","\n","push_to_hub                    = True                 # set to True to push the model\n","run_name                       = \"cl_1024_tok_30k\"    # branch name (only needed if push_to_hub == True)\n","\n","partially_trained              = False                                         # set to True to continue model training from specific commit hash (model_commit_hash)\n","model_commit_hash              = \"91aac830c5ff8417d8bf389eea271a9d3dabab9c\"    # only needed if partially_trained == True\n","previously_completed_steps     = 2295                                          # only needed if partially_trained == True\n","previous_global_steps          = 18360                                         # only needed if partially_trained == True\n","previous_step                  = 1836                                          # only needed if partially_trained == True\n","stopped_lr                     = lr                                            # only needed if partially_trained == True\n","stopped_epoch                  = 9                                             # only needed if partially_trained == True\n","\n","torch.manual_seed(7)"],"metadata":{"id":"7VRZhQTKZaSs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711582429773,"user_tz":-120,"elapsed":4,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"34d4b08f-37f9-4927-d822-9db3c8b27c2c"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7d3a9c209cf0>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# CUSTOM LOSS FUNCTION\n","def custom_loss(inputs, logits):\n","    # inputs [batch_size X cl]\n","    # logits [btach_size X cl X vocab_size]\n","    # Our labels start from second sequence token because first one does not have preceding token.\n","    # We drop last logit because last sequence token does not have subsequent token, so no label to compare\n","    shifted_labels = inputs[..., 1:].contiguous()\n","    shifted_logits = logits[..., :-1, :].contiguous()\n","\n","    loss_func = CrossEntropyLoss(reduce=False)\n","    # loss [batch_size * (cl-1)] = loss_fct([batch_size * (cl-1) X vocab_size], [batch_size * (cl-1)])\n","    loss = loss_func(shifted_logits.view(-1, shifted_logits.size(-1)), shifted_labels.view(-1))\n","    # loss_per_sample [batch_size]\n","    loss_per_sequence = loss.view(shifted_logits.size(0), shifted_logits.size(1)).mean(axis=1)\n","    return loss_per_sequence.mean()"],"metadata":{"id":"dVnxDLITeUwq","executionInfo":{"status":"ok","timestamp":1711582429773,"user_tz":-120,"elapsed":3,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# PARAMETERS FOR WEIGHT DECAY\n","def get_wd_parameters(model, no_decay=[\"bias\", r\"ln_.{1,2}\\.weight\"]):\n","    wd_params = []\n","    nwd_params = []\n","    for name, params in model.named_parameters():\n","        if any(re.search(nd_reg, name) for nd_reg in no_decay):\n","            nwd_params.append(params)\n","        else:\n","            wd_params.append(params)\n","    return [\n","        {\"params\": wd_params,  \"weight_decay\": weight_decay},\n","        {\"params\": nwd_params, \"weight_decay\": 0.0},\n","    ]"],"metadata":{"id":"s4G7PyR8eaZV","executionInfo":{"status":"ok","timestamp":1711582429773,"user_tz":-120,"elapsed":3,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# TOKENIZE RAW DATASETS\n","def get_tokenized_dataset(p_raw_dataset, p_context_length, p_tokenizer):\n","    concatenated_tokenized_samples = []\n","    for sample in p_raw_dataset:\n","        tokenized_sample = p_tokenizer(sample[\"content\"], truncation=False)[\"input_ids\"]\n","        concatenated_tokenized_samples.extend(tokenized_sample + [p_tokenizer.eos_token_id])\n","    tokenized_dataset_list = []\n","    for i in range(0, len(concatenated_tokenized_samples), p_context_length):\n","        input_ids = concatenated_tokenized_samples[i : i + p_context_length]\n","        if len(input_ids) == p_context_length:\n","            tokenized_dataset_list.append(torch.tensor(input_ids))\n","\n","    return Dataset.from_dict({\"input_ids\": tokenized_dataset_list})"],"metadata":{"id":"ZHTy8qSxLjjJ","executionInfo":{"status":"ok","timestamp":1711582429773,"user_tz":-120,"elapsed":3,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# SAVING TRAINING RESULTS TO THE JSON FILE\n","def save_results(filepath:str, split: str, results: dict):\n","    \"\"\"\n","    {\n","        \"train\": [\n","            results1,\n","            results2,\n","            ...\n","        ],\n","        \"valid\": [\n","            results1,\n","            results2,\n","            ...\n","        ]\n","    }\n","    \"\"\"\n","    if not save_json_logs:\n","        return\n","    if not os.path.exists(tensorboard_run_path)\n","        print(\"ERROR: filepath DOES NOT EXIST\")\n","        return\n","    if split not in {\"train\", \"valid\"}:\n","        print(\"ERROR: INVALID SPLIT\")\n","        return\n","    json_data = {\"train\": [], \"valid\": []}\n","    if os.path.exists(filepath):\n","        with open(filepath, 'r') as json_file:\n","            json_data = json.load(json_file)\n","    json_data[split].append(results)\n","    with open(filepath, 'w') as json_file:\n","        json.dump(json_data, json_file, indent=4)"],"metadata":{"id":"3dJLZFnz2StA","executionInfo":{"status":"ok","timestamp":1711582429773,"user_tz":-120,"elapsed":2,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# TENSORBOARD VISUALISATION\n","def add_to_tensorboard(json_filepath, tensorboard_run_path):\n","    if not save_tensorboard_logs:\n","        return\n","    if not os.path.exists(json_filepath)\n","        print(\"ERROR: json_filepath DOES NOT EXIST\")\n","        return\n","    if not os.path.exists(tensorboard_run_path)\n","        print(\"ERROR: tensorboard_run_path DOES NOT EXIST\")\n","        return\n","    with open(json_filepath, mode=\"r\") as json_file:\n","        one_step_tokens = entry[\"hyperparams\"][\"train_batch_size\"] * entry[\"hyperparams\"][\"gradient_accumulation_steps\"] * entry[\"hyperparams\"][\"context_length\"]\n","        json_data = json.load(json_file)\n","        writer = SummaryWriter(tensorboard_run_path)\n","        for entry in json_data[\"train\"]:\n","            writer.add_scalar(\"Loss/Train\", entry[\"loss/train\"], entry[\"competed_steps\"] * one_step_tokens)\n","            writer.add_scalar(\"Learning Rate\", entry[\"lr\"][0], entry[\"competed_steps\"] * one_step_tokens)\n","        for entry in json_data[\"valid\"]:\n","            writer.add_scalar(\"Loss/Eval\", entry[\"loss/eval\"], entry[\"competed_steps\"] * one_step_tokens)\n","            writer.add_scalar(\"Perplexity/Eval\", entry[\"perplexity\"], entry[\"competed_steps\"] * one_step_tokens)\n","      writer.close()"],"metadata":{"id":"negAfxKOtdOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### CONFIGURING"],"metadata":{"id":"1avaSDQZ_-M2"}},{"cell_type":"code","source":["# MOUNTING DRIVE FOR SAVING LOGS\n","if drive_mounted:\n","    drive.mount(drive_mounted_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0mp6KpOhAC2T","executionInfo":{"status":"ok","timestamp":1711582433859,"user_tz":-120,"elapsed":3025,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"68cac8fd-cd70-405e-93a8-2b6c0c4b87ea"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# CONFIGURING GIT CREDENTIALS\n","if have_git_write_access:\n","    !git config --global user.email \"{user_email}\"\n","    !git config --global user.name \"{user_name}\"\n","\n","# To set Hugging Face token (for writing access) create HF_TOKEN secret in Google Collab or use notebook_login()"],"metadata":{"id":"5Z8pOm7N_hql","executionInfo":{"status":"ok","timestamp":1711582433859,"user_tz":-120,"elapsed":4,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# CONFIGURING GIT DIRECTORIES\n","if push_to_hub:\n","    repo = Repository(model_output_dir, clone_from=model_repo_name)\n","    repo.git_checkout(run_name, create_branch_ok=True)"],"metadata":{"id":"xxYRTff8gOzr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711582450569,"user_tz":-120,"elapsed":3229,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"82c7723f-c3b7-402d-c435-4780f50c669a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n","For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n","  warnings.warn(warning_message, FutureWarning)\n","/content/./gpt2-pretrained-for-coq-pt-custom-train-local is already a clone of https://huggingface.co/Andrusyshyn/gpt2-pretrained-for-coq-pt-custom-train. Make sure you pull the latest changes with `repo.git_pull()`.\n","WARNING:huggingface_hub.repository:/content/./gpt2-pretrained-for-coq-pt-custom-train-local is already a clone of https://huggingface.co/Andrusyshyn/gpt2-pretrained-for-coq-pt-custom-train. Make sure you pull the latest changes with `repo.git_pull()`.\n","Checked out cl_1024_tok_30k from cl_1024_tok_30k.\n","WARNING:huggingface_hub.repository:Checked out cl_1024_tok_30k from cl_1024_tok_30k.\n","M\tconfig.json\n","M\tgeneration_config.json\n","M\tmerges.txt\n","M\tmodel.safetensors\n","M\ttokenizer.json\n","M\tvocab.json\n","\n","WARNING:huggingface_hub.repository:M\tconfig.json\n","M\tgeneration_config.json\n","M\tmerges.txt\n","M\tmodel.safetensors\n","M\ttokenizer.json\n","M\tvocab.json\n","\n"]}]},{"cell_type":"markdown","source":["#### DATASETS"],"metadata":{"id":"QCzfmzzIgSXC"}},{"cell_type":"code","source":["# UNPACK DATASETS\n","if data_archived:\n","    !gzip -dkv \"{raw_train_archive}\"\n","    !gzip -dkv \"{raw_valid_archive}\"\n","    raw_train_json = raw_train_archive[:-3]\n","    raw_valid_json = raw_valid_archive[:-3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DyIeffM-P9if","executionInfo":{"status":"ok","timestamp":1711580382475,"user_tz":-120,"elapsed":836,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"4ed9f8a0-591c-4c7a-c089-1653176c6ef1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["./pretrain_dataset_train.json.gz:\t 82.6% -- created ./pretrain_dataset_train.json\n","./pretrain_dataset_validation.json.gz:\t 80.5% -- created ./pretrain_dataset_validation.json\n"]}]},{"cell_type":"code","source":["# LOAD DATASETS\n","data_files = {\"train\": raw_train_json, \"validation\": raw_valid_json}\n","raw_datasets = load_dataset(\"json\", data_files=data_files, field=\"data\")\n","print(raw_datasets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtwDIj4TP-NP","executionInfo":{"status":"ok","timestamp":1711582468475,"user_tz":-120,"elapsed":1432,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"88f192a4-bd8d-4848-ff09-6a1d242091cb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['filepath', 'content'],\n","        num_rows: 5969\n","    })\n","    validation: Dataset({\n","        features: ['filepath', 'content'],\n","        num_rows: 663\n","    })\n","})\n"]}]},{"cell_type":"code","source":["# DEBUG CODE (taking few samples from whole DatasetDict)\n","train_debug = raw_datasets[\"train\"][:100]\n","valid_debug = raw_datasets[\"validation\"][:3]\n","train_debug_dataset = Dataset.from_dict(train_debug)\n","valid_debug_dataset = Dataset.from_dict(valid_debug)\n","raw_datasets = DatasetDict({\"train\": train_debug_dataset, \"validation\": raw_datasets[\"validation\"]})\n","print(raw_datasets)"],"metadata":{"id":"m3xbNM7ajC5A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710592471905,"user_tz":-120,"elapsed":273,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"c6628826-0875-4019-bd00-c0956a734daa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['content', 'filepath'],\n","        num_rows: 100\n","    })\n","    validation: Dataset({\n","        features: ['content', 'filepath'],\n","        num_rows: 12401\n","    })\n","})"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# LOADING TOKENIZER\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_name, revision=tokenizer_commit_hash)\n","print(\"tokenizer vocab size: \", len(tokenizer))\n","# TOKENIZE RAW DATASETS\n","train_dataset = get_tokenized_dataset(raw_datasets[\"train\"],      context_length, tokenizer)\n","valid_dataset = get_tokenized_dataset(raw_datasets[\"validation\"], context_length, tokenizer)\n","train_dataset.set_format(\"torch\")\n","valid_dataset.set_format(\"torch\")\n","\n","# CREATE DATALOADERS\n","train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=eval_batch_size)\n","print(train_dataset)\n","print(valid_dataset)\n","print(\"len(train_dataloader): \", len(train_dataloader))\n","print(\"len(valid_dataloader): \", len(valid_dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EBqU_TnKQBiX","executionInfo":{"status":"ok","timestamp":1711582589719,"user_tz":-120,"elapsed":111825,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"bda7994d-0822-4f61-bea6-ddffd5dd669f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (5887 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["tokenizer vocab size:  30000\n","Dataset({\n","    features: ['input_ids'],\n","    num_rows: 22643\n","})\n","Dataset({\n","    features: ['input_ids'],\n","    num_rows: 2784\n","})\n","len(train_dataloader):  5661\n","len(valid_dataloader):  696\n"]}]},{"cell_type":"markdown","source":["#### CONFIGURING MODEL AND TRAINING"],"metadata":{"id":"uozaVBJNiCFw"}},{"cell_type":"code","source":["# PRE TRAINING MODEL FROM SCRATCH!\n","if not partially_trained:\n","    config = AutoConfig.from_pretrained(\n","        init_model,\n","        vocab_size=len(tokenizer),\n","        n_ctx=context_length,\n","        bos_token_id=tokenizer.bos_token_id,\n","        eos_token_id=tokenizer.eos_token_id,\n","    )\n","\n","    model = GPT2LMHeadModel(config)\n","    if gradient_checkpointing:\n","        model.gradient_checkpointing_enable()\n","    optimizer = AdamW(get_wd_parameters(model), lr=lr)\n","\n","    accelerator = Accelerator(mixed_precision=mixed_precision)\n","\n","    model, optimizer, train_dataloader, valid_dataloader = accelerator.prepare(\n","        model, optimizer, train_dataloader, valid_dataloader\n","    )\n","\n","    print(f\"Model size:                                        {model.num_parameters()}\")\n","    print(f\"Model size (only trainable params):                {model.num_parameters(only_trainable=True)}\")\n","    print(f\"Model size (only trainable non-embeddings params): {model.num_parameters(only_trainable=True, exclude_embeddings=True)}\")"],"metadata":{"id":"nxQPSwfzN7IQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711582593501,"user_tz":-120,"elapsed":3792,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"777b7b95-8ac5-481d-c306-8b36f53045fb"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model size:                                        108882432\n","Model size (only trainable params):                108882432\n","Model size (only trainable non-embeddings params): 85056000\n"]}]},{"cell_type":"code","source":["# PRE TRAINING MODEL FROM SCRATCH!\n","if not partially_trained:\n","    num_steps_per_epoch = len(train_dataloader)\n","    print(\"Num steps per epoch: \", num_steps_per_epoch)\n","    num_training_completed_steps = (num_train_epochs * num_steps_per_epoch) // gradient_accumulation_steps\n","    if ((num_train_epochs * num_steps_per_epoch) % gradient_accumulation_steps != 0):\n","        num_training_completed_steps += 1\n","    print(\"Num optimizer steps: \", num_training_completed_steps)\n","\n","    lr_scheduler = get_scheduler(\n","        name=\"linear\",\n","        optimizer=optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_training_completed_steps\n","    )"],"metadata":{"id":"5JPdPbOhOCS_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711582593501,"user_tz":-120,"elapsed":4,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"2a3fac8b-292f-4c2e-d701-287b7849ee1e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Num steps per epoch:  5661\n","Num optimizer steps:  3539\n"]}]},{"cell_type":"code","source":["# USING ALREADY TRAINED MODEL!!!\n","if partially_trained:\n","    num_steps_per_epoch = len(train_dataloader)\n","    num_training_completed_steps = (num_train_epochs * num_steps_per_epoch) // gradient_accumulation_steps\n","    if ((num_train_epochs * num_steps_per_epoch) % gradient_accumulation_steps != 0):\n","        num_training_completed_steps += 1\n","\n","    lr_sch_warmup_steps = 0\n","    num_training_completed_steps -= previously_completed_steps\n","    lr_sch_lr = stopped_lr\n","\n","    print(\"Num optimizer steps: \", num_training_completed_steps)\n","    print(\"Start Learning rate: \", lr_sch_lr)\n","    print(\"Warmup steps: \", lr_sch_warmup_steps)\n","    ################################################################################\n","    model = GPT2LMHeadModel.from_pretrained(model_repo_name, revision=model_commit_hash)\n","    if gradient_checkpointing:\n","        model.gradient_checkpointing_enable()\n","    optimizer = AdamW(get_wd_parameters(model), lr=lr_sch_lr)\n","    accelerator = Accelerator(mixed_precision=mixed_precision)\n","    model, optimizer, train_dataloader, valid_dataloader = accelerator.prepare(\n","        model, optimizer, train_dataloader, valid_dataloader\n","    )\n","\n","    lr_scheduler = get_scheduler(\n","        name=\"linear\",\n","        optimizer=optimizer,\n","        num_warmup_steps=lr_sch_warmup_steps,\n","        num_training_steps=num_training_completed_steps\n","    )\n","\n","    print()\n","    print(f\"Model size:                                        {model.num_parameters()}\")\n","    print(f\"Model size (only trainable params):                {model.num_parameters(only_trainable=True)}\")\n","    print(f\"Model size (only trainable non-embeddings params): {model.num_parameters(only_trainable=True, exclude_embeddings=True)}\")"],"metadata":{"id":"lK-AhF5NwiEd","executionInfo":{"status":"ok","timestamp":1711582593501,"user_tz":-120,"elapsed":3,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# EVALUATION FUNCTION\n","def evaluate():\n","    torch.cuda.empty_cache()\n","\n","    model.eval()\n","    losses = []\n","    with torch.no_grad():\n","        for step, batch in enumerate(valid_dataloader):\n","            with torch.no_grad():\n","                logits = model(batch[\"input_ids\"]).logits\n","                loss = custom_loss(batch[\"input_ids\"], logits)\n","                losses.append(loss.item())\n","    loss = torch.mean(torch.Tensor(losses))\n","    try:\n","        perplexity = torch.exp(loss)\n","    except OverflowError:\n","        perplexity = float(\"inf\")\n","\n","    torch.cuda.empty_cache()\n","    return loss.item(), perplexity.item()"],"metadata":{"id":"hK4sg5C3eybv","executionInfo":{"status":"ok","timestamp":1711582593501,"user_tz":-120,"elapsed":3,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nzymk9udYzpw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TRAINING LOOP\n","model.train()\n","completed_steps = 0\n","global_steps = 0\n","if partially_trained:\n","    completed_steps = previously_completed_steps\n","    global_steps = previous_global_steps\n","for epoch in range(num_train_epochs):\n","    for step, batch in tqdm(\n","        enumerate(train_dataloader, start=0), total=num_steps_per_epoch\n","    ):\n","        if partially_trained and ((epoch<stopped_epoch) or ((epoch==stopped_epoch) and (step <= previous_step))):\n","            continue\n","\n","        logits = model(batch[\"input_ids\"]).logits\n","        loss = custom_loss(batch[\"input_ids\"], logits)\n","################################################################################\n","        if (global_steps % ((int(0.1 * num_steps_per_epoch) // gradient_accumulation_steps) * gradient_accumulation_steps) == 0):\n","            log_train = {\n","                    \"epoch\": epoch,\n","                    \"lr\": lr_scheduler.get_lr(),\n","                    \"steps\": step,\n","                    \"global_steps\" : global_steps,\n","                    \"competed_steps\": completed_steps,\n","                    \"loss/train\": loss.item(),\n","            }\n","            accelerator.print(log_train)\n","            save_results(drive_train_res_path, \"train\", log_train)\n","################################################################################\n","        loss = loss / gradient_accumulation_steps\n","        accelerator.backward(loss)\n","        global_steps += 1\n","################################################################################\n","        if global_steps % gradient_accumulation_steps == 0:\n","            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            completed_steps += 1\n","        if (global_steps % (eval_steps * gradient_accumulation_steps)) == 0:\n","            eval_loss, perplexity = evaluate()\n","            log_eval = {\n","                    \"loss/eval\": eval_loss,\n","                    \"perplexity\": perplexity,\n","                    \"epoch\": epoch,\n","                    \"lr\": lr_scheduler.get_lr(),\n","                    \"steps\": step,\n","                    \"global_steps\" : global_steps,\n","                    \"competed_steps\": completed_steps,\n","                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n","            }\n","            accelerator.print(log_eval)\n","            log_train = {\n","                    \"epoch\": epoch,\n","                    \"lr\": lr_scheduler.get_lr(),\n","                    \"steps\": step,\n","                    \"global_steps\" : global_steps-1,\n","                    \"competed_steps\": completed_steps-1,\n","                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n","            }\n","            save_results(drive_train_res_path, \"train\", log_train)\n","            save_results(drive_train_res_path, \"valid\", log_eval)\n","            accelerator.wait_for_everyone()\n","            unwrapped_model = accelerator.unwrap_model(model)\n","            unwrapped_model.save_pretrained(model_output_dir, save_function=accelerator.save)\n","            tokenizer.save_pretrained(model_output_dir)\n","            if accelerator.is_main_process:\n","                if push_to_hub:\n","                    repo.push_to_hub(\n","                        commit_message=f\"Training in progress epoch {epoch}; steps {step}; global_steps {global_steps}; completed_steps {completed_steps}; lr {lr_scheduler.get_lr()}; loss/train {loss.item() * gradient_accumulation_steps}; loss/eval {eval_loss}; perplexity {perplexity}\", blocking=False    # train loss is for the next step\n","                    )\n","            model.train()\n","\n","################################################################################\n","################################################################################\n","################################################################################\n","\n","#GRADIENT UPDATE (In case (global_steps % gradient_accumulation_steps != 0)\n","if (global_steps % gradient_accumulation_steps != 0):\n","    for step, batch in tqdm(\n","        enumerate(train_dataloader, start=0), total=(gradient_accumulation_steps - (global_steps % gradient_accumulation_steps)) - 1\n","    ):\n","        logits = model(batch[\"input_ids\"]).logits\n","        loss = custom_loss(batch[\"input_ids\"], logits)\n","        loss = loss / gradient_accumulation_steps\n","        accelerator.backward(loss)\n","        global_steps += 1\n","        if global_steps % gradient_accumulation_steps == 0:\n","            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            completed_steps += 1\n","            break\n","\n","# FINAL EVALUATE AND SAVE\n","additional_steps = 0\n","if (global_steps % gradient_accumulation_steps != 0):\n","    additional_steps = gradient_accumulation_steps - (global_steps % gradient_accumulation_steps)\n","with torch.no_grad():\n","    last_train_loss = 0\n","    for batch in train_dataloader:\n","        logits = model(batch[\"input_ids\"]).logits\n","        loss = custom_loss(batch[\"input_ids\"], logits)\n","        last_train_loss = loss.item()\n","        break\n","eval_loss, perplexity = evaluate()\n","log_eval = {\n","        \"loss/eval\": eval_loss,\n","        \"perplexity\": perplexity,\n","        \"epoch\": epoch,\n","        \"lr\": lr_scheduler.get_lr(),\n","        \"steps\": num_steps_per_epoch + additional_steps,\n","        \"global_steps\" : global_steps,\n","        \"competed_steps\": completed_steps,\n","        \"loss/train\": last_train_loss,\n","}\n","accelerator.print(log_eval)\n","log_train = {\n","        \"epoch\": epoch,\n","        \"lr\": lr_scheduler.get_lr(),\n","        \"steps\": num_steps_per_epoch + additional_steps,\n","        \"global_steps\" : global_steps,\n","        \"competed_steps\": completed_steps,\n","        \"loss/train\": last_train_loss,\n","}\n","save_results(drive_train_res_path, \"train\", log_train)\n","save_results(drive_train_res_path, \"valid\", log_eval)\n","accelerator.wait_for_everyone()\n","unwrapped_model = accelerator.unwrap_model(model)\n","unwrapped_model.save_pretrained(model_output_dir, save_function=accelerator.save)\n","tokenizer.save_pretrained(model_output_dir)\n","if accelerator.is_main_process:\n","    if push_to_hub:\n","        repo.push_to_hub(\n","            commit_message=f\"Final model: epoch {epoch}; steps {num_steps_per_epoch+additional_steps}; global_steps {global_steps}; completed_steps {completed_steps}; lr {lr_scheduler.get_lr()}; loss/train {last_train_loss}; loss/eval {eval_loss}; perplexity {perplexity}\", blocking=False\n","        )\n","model.train()\n","\n","torch.cuda.empty_cache()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["febf83b414de40bfacb03e44cc32017a","b058a30805df4dcd979274c0a937be9c","2635229f427d4a3d9a3f5cc5d5a78a6e","119ee85adc814147bcea0201454f62b2","3b2750ebf3c544989fd99ed60ab976d4","05e94ea5af3149f48817ae96b79ca6ab","89b9daa9adba425fa41fb825dfc3b5ff","5497e12b059249e490755d11fdbef9e7","f8123a5252c84e8ba1b6963a7fd154d1","39802cced75449e49e8ac3289576d3c4","a9e87d8c653f4d3496eb09ca49c3b1a0","958dfde2e2b245c89f4c9b133449806b","b7eeb849972143adaf61dac857415e22","92498eea1caa41809497ec442c9090e4","df0c2020db7e439f8d167510cdcd36da","1ebd954e05b24234b26b9287f1e71c8a","0b6b8d7ca9db4a1aa7b5c11539ef9264","c89d8519c6094258ad357f79f53ae035","439e4a1ebfe44af5bd6236bfc8e4981e","48e167315bf447bba1f0c23b0fcc71b6","fa3e209a4129447f920d421b9ac0ed22","02b5392e5dd4430b87a5198fc40b0c46","eb9b4756bf804b0f9879f003fd24fbfc","0ff9040a0494408aa5a98563953b07cd","1748549ab76b45fc848858cfbf568797","dcbbacafab1c46a38facbab1bb51d719","57ae7579a5b9426fad84796940a6f133","ac3f6cf2ada2449c8c91ab8d5abaaff6","49de93d47afc41e18dce25d477925400","c5fa25a4b7ad493080c080220295a1f6","558de3458ac1471f8e7670a715573c3d","faa022e4668f40f8b369f9e9b1cb61ca","fbb4fab7a4d64169957f3c50934478ad"]},"id":"CMxa25rIPbAi","outputId":"9ba1ca9f-5dd9-4e53-8e44-6641a85b2bf2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5661 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"febf83b414de40bfacb03e44cc32017a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 0, 'lr': [0.0, 0.0], 'steps': 0, 'global_steps': 0, 'competed_steps': 0, 'loss/train': 10.455289840698242}\n","{'epoch': 0, 'lr': [0.00049713384924047, 0.00049713384924047], 'steps': 560, 'global_steps': 560, 'competed_steps': 70, 'loss/train': 4.529292106628418}\n","{'loss/eval': 4.685021877288818, 'perplexity': 108.3126449584961, 'epoch': 0, 'lr': [0.0004892519346517627, 0.0004892519346517627], 'steps': 999, 'global_steps': 1000, 'competed_steps': 125, 'loss/train': 5.145664215087891}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 0, 'lr': [0.0004871023215821152, 0.0004871023215821152], 'steps': 1120, 'global_steps': 1120, 'competed_steps': 140, 'loss/train': 4.042849063873291}\n","{'epoch': 0, 'lr': [0.0004770707939237604, 0.0004770707939237604], 'steps': 1680, 'global_steps': 1680, 'competed_steps': 210, 'loss/train': 4.144962310791016}\n","{'loss/eval': 4.058959484100342, 'perplexity': 57.91402053833008, 'epoch': 0, 'lr': [0.00047133849240470053, 0.00047133849240470053], 'steps': 1999, 'global_steps': 2000, 'competed_steps': 250, 'loss/train': 2.824413299560547}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (2) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (2) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 0, 'lr': [0.00046703926626540554, 0.00046703926626540554], 'steps': 2240, 'global_steps': 2240, 'competed_steps': 280, 'loss/train': 3.851320266723633}\n","{'epoch': 0, 'lr': [0.00045700773860705073, 0.00045700773860705073], 'steps': 2800, 'global_steps': 2800, 'competed_steps': 350, 'loss/train': 3.7508034706115723}\n","{'loss/eval': 3.7102177143096924, 'perplexity': 40.862701416015625, 'epoch': 0, 'lr': [0.00045342505015763827, 0.00045342505015763827], 'steps': 2999, 'global_steps': 3000, 'competed_steps': 375, 'loss/train': 2.9665074348449707}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (3) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (3) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 0, 'lr': [0.0004469762109486959, 0.0004469762109486959], 'steps': 3360, 'global_steps': 3360, 'competed_steps': 420, 'loss/train': 3.7038979530334473}\n","{'epoch': 0, 'lr': [0.0004369446832903411, 0.0004369446832903411], 'steps': 3920, 'global_steps': 3920, 'competed_steps': 490, 'loss/train': 2.532991647720337}\n","{'loss/eval': 3.3967089653015137, 'perplexity': 29.86564826965332, 'epoch': 0, 'lr': [0.00043551160791057607, 0.00043551160791057607], 'steps': 3999, 'global_steps': 4000, 'competed_steps': 500, 'loss/train': 2.9878158569335938}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (4) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (4) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 0, 'lr': [0.00042691315563198626, 0.00042691315563198626], 'steps': 4480, 'global_steps': 4480, 'competed_steps': 560, 'loss/train': 2.4860339164733887}\n","{'loss/eval': 3.169886827468872, 'perplexity': 23.804790496826172, 'epoch': 0, 'lr': [0.00041759816566351387, 0.00041759816566351387], 'steps': 4999, 'global_steps': 5000, 'competed_steps': 625, 'loss/train': 3.2624502182006836}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (5) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (5) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 0, 'lr': [0.00041688162797363145, 0.00041688162797363145], 'steps': 5040, 'global_steps': 5040, 'competed_steps': 630, 'loss/train': 2.639338493347168}\n","{'epoch': 0, 'lr': [0.0004068501003152766, 0.0004068501003152766], 'steps': 5600, 'global_steps': 5600, 'competed_steps': 700, 'loss/train': 2.9396209716796875}\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5661 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"958dfde2e2b245c89f4c9b133449806b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'loss/eval': 2.9767768383026123, 'perplexity': 19.624462127685547, 'epoch': 1, 'lr': [0.0003996847234164517, 0.0003996847234164517], 'steps': 338, 'global_steps': 6000, 'competed_steps': 750, 'loss/train': 2.6845688819885254}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (6) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (6) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 1, 'lr': [0.0003968185726569218, 0.0003968185726569218], 'steps': 499, 'global_steps': 6160, 'competed_steps': 770, 'loss/train': 2.786147117614746}\n","{'epoch': 1, 'lr': [0.0003867870449985669, 0.0003867870449985669], 'steps': 1059, 'global_steps': 6720, 'competed_steps': 840, 'loss/train': 2.0801854133605957}\n","{'loss/eval': 2.820486068725586, 'perplexity': 16.78500747680664, 'epoch': 1, 'lr': [0.0003817712811693895, 0.0003817712811693895], 'steps': 1338, 'global_steps': 7000, 'competed_steps': 875, 'loss/train': 1.8183856010437012}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (7) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (7) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 1, 'lr': [0.0003767555173402121, 0.0003767555173402121], 'steps': 1619, 'global_steps': 7280, 'competed_steps': 910, 'loss/train': 1.999011754989624}\n","{'epoch': 1, 'lr': [0.00036672398968185725, 0.00036672398968185725], 'steps': 2179, 'global_steps': 7840, 'competed_steps': 980, 'loss/train': 1.9623982906341553}\n","{'loss/eval': 2.6545791625976562, 'perplexity': 14.219000816345215, 'epoch': 1, 'lr': [0.0003638578389223273, 0.0003638578389223273], 'steps': 2338, 'global_steps': 8000, 'competed_steps': 1000, 'loss/train': 1.5572048425674438}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (8) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (8) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 1, 'lr': [0.00035669246202350244, 0.00035669246202350244], 'steps': 2739, 'global_steps': 8400, 'competed_steps': 1050, 'loss/train': 1.7646706104278564}\n","{'epoch': 1, 'lr': [0.00034666093436514764, 0.00034666093436514764], 'steps': 3299, 'global_steps': 8960, 'competed_steps': 1120, 'loss/train': 1.536243200302124}\n","{'loss/eval': 2.5040295124053955, 'perplexity': 12.231682777404785, 'epoch': 1, 'lr': [0.0003459443966752651, 0.0003459443966752651], 'steps': 3338, 'global_steps': 9000, 'competed_steps': 1125, 'loss/train': 1.5786935091018677}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (9) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (9) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 1, 'lr': [0.0003366294067067928, 0.0003366294067067928], 'steps': 3859, 'global_steps': 9520, 'competed_steps': 1190, 'loss/train': 2.210226535797119}\n","{'loss/eval': 2.3795740604400635, 'perplexity': 10.800301551818848, 'epoch': 1, 'lr': [0.0003280309544282029, 0.0003280309544282029], 'steps': 4338, 'global_steps': 10000, 'competed_steps': 1250, 'loss/train': 2.3846771717071533}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (10) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (10) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 1, 'lr': [0.00032659787904843797, 0.00032659787904843797], 'steps': 4419, 'global_steps': 10080, 'competed_steps': 1260, 'loss/train': 2.0058906078338623}\n","{'epoch': 1, 'lr': [0.0003165663513900831, 0.0003165663513900831], 'steps': 4979, 'global_steps': 10640, 'competed_steps': 1330, 'loss/train': 1.5822627544403076}\n","{'loss/eval': 2.214085817337036, 'perplexity': 9.153038024902344, 'epoch': 1, 'lr': [0.00031011751218114076, 0.00031011751218114076], 'steps': 5338, 'global_steps': 11000, 'competed_steps': 1375, 'loss/train': 1.9863076210021973}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (11) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (11) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 1, 'lr': [0.0003065348237317283, 0.0003065348237317283], 'steps': 5539, 'global_steps': 11200, 'competed_steps': 1400, 'loss/train': 2.1521151065826416}\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5661 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb9b4756bf804b0f9879f003fd24fbfc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'epoch': 2, 'lr': [0.00029650329607337344, 0.00029650329607337344], 'steps': 438, 'global_steps': 11760, 'competed_steps': 1470, 'loss/train': 2.1484994888305664}\n","{'loss/eval': 2.1116554737091064, 'perplexity': 8.261907577514648, 'epoch': 2, 'lr': [0.00029220406993407856, 0.00029220406993407856], 'steps': 677, 'global_steps': 12000, 'competed_steps': 1500, 'loss/train': 1.9952863454818726}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (12) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (12) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 2, 'lr': [0.00028647176841501863, 0.00028647176841501863], 'steps': 998, 'global_steps': 12320, 'competed_steps': 1540, 'loss/train': 2.0682852268218994}\n","{'epoch': 2, 'lr': [0.0002764402407566638, 0.0002764402407566638], 'steps': 1558, 'global_steps': 12880, 'competed_steps': 1610, 'loss/train': 1.5739586353302002}\n","{'loss/eval': 2.0176873207092285, 'perplexity': 7.52091121673584, 'epoch': 2, 'lr': [0.00027429062768701635, 0.00027429062768701635], 'steps': 1677, 'global_steps': 13000, 'competed_steps': 1625, 'loss/train': 1.5805190801620483}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (13) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (13) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 2, 'lr': [0.000266408713098309, 0.000266408713098309], 'steps': 2118, 'global_steps': 13440, 'competed_steps': 1680, 'loss/train': 1.3008208274841309}\n","{'loss/eval': 1.9573551416397095, 'perplexity': 7.080574989318848, 'epoch': 2, 'lr': [0.00025637718543995415, 0.00025637718543995415], 'steps': 2677, 'global_steps': 14000, 'competed_steps': 1750, 'loss/train': 2.3755359649658203}\n"]},{"output_type":"stream","name":"stderr","text":["Several commits (14) will be pushed upstream.\n","WARNING:huggingface_hub.repository:Several commits (14) will be pushed upstream.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["{'epoch': 2, 'lr': [0.00025637718543995415, 0.00025637718543995415], 'steps': 2678, 'global_steps': 14000, 'competed_steps': 1750, 'loss/train': 1.3428977727890015}\n"]}]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","add_to_tensorboard(drive_train_res_path, tensorboard_run_path)"],"metadata":{"id":"fuzcHch1MWt3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir \"{tensorboard_run_path}\""],"metadata":{"id":"ccAmPDh7pwaI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TESTING PRETRAINED MODEL"],"metadata":{"id":"9vi6XSOUTtO8"}},{"cell_type":"code","source":["coq_model = GPT2LMHeadModel.from_pretrained(\"Andrusyshyn/gpt2-pretrained-for-coq-pt-custom-train\")\n","coq_tokenizer = AutoTokenizer.from_pretrained(\"Andrusyshyn/gpt2-coq-tokenizer\")\n","pipe = pipeline(\n","    \"text-generation\", model=coq_model, tokenizer=coq_tokenizer#, device=0\n",")"],"metadata":{"id":"IU005nHITxB5","colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"status":"error","timestamp":1708957870721,"user_tz":-120,"elapsed":1639,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"80c3b34e-f2b2-4022-a2e3-af061bafcbee"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-5687d943fba1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcoq_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Andrusyshyn/gpt2-pretrained-for-coq-pt-custom-train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcoq_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Andrusyshyn/gpt2-coq-tokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m pipe = pipeline(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoq_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoq_tokenizer\u001b[0m\u001b[0;31m#, device=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3848\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3849\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3850\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3851\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3852\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4223\u001b[0m                 \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4224\u001b[0m             )\n\u001b[0;32m-> 4225\u001b[0;31m             \u001b[0merror_msgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_state_dict_into_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4226\u001b[0m             \u001b[0moffload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;31m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;31m# it's safe to delete it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    619\u001b[0m                             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                                 \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m                             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m                     error_msgs.append(f'While copying the parameter named \"{key}\", '\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["model = coq_model\n","evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":388},"id":"yGFokRO3IEOc","executionInfo":{"status":"error","timestamp":1707425966044,"user_tz":-120,"elapsed":6182381,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"b36206a2-74c2-491f-a8c0-3b33a8d0e2b5"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-c9e222b9a73f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoq_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-fed43d284866>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# print(batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# print(outputs.loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# print(accelerator.gather(outputs.loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0;31m# Flatten the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["txt = \"\"\"Theorem identity : forall (n : nat), n = n.\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rd1khsdT66h","executionInfo":{"status":"ok","timestamp":1707121104711,"user_tz":-120,"elapsed":3176,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"96bf82d5-3d0c-4135-e623-881d08216c96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Theorem identity : forall (n : nat), n = n.\n","intros; red; intros; destruct n; simpl; auto.\n","Qed.\n","\n","Theorem eq_sym : forall X : Ens, IN X X H.\n","intros;\n"]}]},{"cell_type":"markdown","source":["### SANDBOX"],"metadata":{"id":"_srRfnAxr_cG"}},{"cell_type":"code","source":["class CustomStoppingCriteria(StoppingCriteria):\n","    def __init__(self, stop_sequences):\n","        super().__init__()\n","        self.stop_sequences = stop_sequences\n","\n","    def __call__(self, input_ids, scores, **kwargs):\n","        # print(input_ids)\n","        # Check if the generated text ends with any of the stop sequences\n","        for sequence in self.stop_sequences:\n","            subsequence = input_ids[0][-len(sequence):]\n","            # print(\"###############################################################\")\n","            # print(sequence, subsequence)\n","            if (sequence == subsequence.tolist()):\n","                # print(\"RETURN TRUEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\")\n","                return True  # Stop criteria met\n","        return False  # Stop criteria not met"],"metadata":{"id":"uIOwx6pE1RFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# stop_sequences = [['ĠQed', '.'], ['Qed', '.']]  # Example stop sequences\n","stop_sequences = [[372, 14]]\n","custom_stopping_criteria = CustomStoppingCriteria(stop_sequences)\n","stopping_criteria = StoppingCriteriaList([custom_stopping_criteria])"],"metadata":{"id":"SdauXZsT1TUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for stop_seq in stop_sequences:\n","  print(tokenizer.encode(stop_seq))"],"metadata":{"id":"-SQLTM2T2wWW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode('ĠQed'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlL0OaD62-oI","executionInfo":{"status":"ok","timestamp":1708958426648,"user_tz":-120,"elapsed":2,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"ede025b4-cc54-4796-e585-da9d5c1b338b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[129, 255, 372]\n"]}]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"Andrusyshyn/gpt2-coq-tokenizer\")\n","model = GPT2LMHeadModel.from_pretrained(\"Andrusyshyn/gpt2-pretrained-for-coq-pt-custom-train\")\n","pipe = pipeline(\n","    \"text-generation\", model=model, tokenizer=tokenizer, stopping_criteria=stopping_criteria#, device=0\n",")"],"metadata":{"id":"oCOQ4GMQsDL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate text\n","generated_text = pipe([\"Theorem theorem1 : forall (n : nat), n + 0 = n.\"], num_return_sequences=50)\n","print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGPCjKzs1Vwu","executionInfo":{"status":"ok","timestamp":1708960585029,"user_tz":-120,"elapsed":19167,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"90fcb798-226d-49ad-d865-30a19411d964"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nauto with arith.\\nQed.'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nHint Immediate plus_reg_l: arith.'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  induction n as IHn as [ |'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro n; case n; auto'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* QuickChick plus_n_O.'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; rewrite mult_comm.\\nrewrite'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n; auto.\\nQed'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintros. omega.\\nQed.'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintro n; case n; auto.\\nsimpl'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nQed.\\n\\nTheorem plus_'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* Higher Order *)\\nAxiom mult_'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintro; omega.\\nQed.\\n\\nLemma'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\ntauto.\\nQed.\\n\\nTheorem le_'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros; omega.\\nQed.\\n\\nTheorem'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n; auto.\\nintros'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.\\nHint Resolve'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n; auto.\\nQed'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  assert (H := le_plus'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.\\nHint Resolve'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintros n; rewrite (plus_'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintro n.\\napply (Build_Map ('}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* OUT+1 n = n *)'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.\\nHint Resolve'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\n\\n  split.\\n  generalize n.\\n  clear n.'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintro; apply (le_Sn_O n'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto.\\nQed.\\nHint Resolve is_'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* Higher Order *)\\n\\nTheorem mult'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  intro; cut (forall q rr'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* Higher Order *)\\n\\nSection NPeano'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.\\nHint Resolve'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\napply (f_equal2 plus'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith. Qed.\\n\\nLemma mult'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nsimpl; auto.\\nQed.\\nHint Resolve'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n; auto.\\nQed'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nreflexivity.\\nQed.\\n\\n\\n\\n\\nLemma mult_'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintros n; rewrite <- (plus'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  intros n; apply n.\\n'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro n; apply le_plus'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n; auto with arith.'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro n; elim n; trivial'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\ncongruence.\\nQed.\\n\\nTheorem mult_'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.\\n \\nTheorem'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintro.\\nsymmetry.\\nrewrite H in H'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* (* numTests 11 *)\\n\\n'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro n; rewrite (plus_'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintros n; simpl; omega.'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* QuickChick plus_0_r.'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro n; apply (K_'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nHint Unfold minus.\\n\\nintro.\\nAdmitted'}, {'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro.\\nnow rewrite (plus'}]]\n"]}]},{"cell_type":"code","source":["for _ in range(50):\n","  generated_text = pipe([\"Theorem theorem1 : forall (n : nat), n + 0 = n.\"], num_return_sequences=1)\n","  print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YAHTELCe-1Nc","executionInfo":{"status":"ok","timestamp":1708960715593,"user_tz":-120,"elapsed":116898,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"84532eef-202d-4d71-febe-cfca199ad63c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n; unfold O in |- *; auto.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nsimpl in |- *; auto.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': \"Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; case n; simpl in |- *; auto.\\nintros n1 H'; right;\\nrewrite H'; auto.\\nunfold nat_of_P\"}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted.\\n\\nTheorem G1 : forall n m, n = m + m.\\nAdmitted. (* Leo: OUT-OFne *)\\n\\nTheorem G'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAbort.\\n\\nGoal forall (n m : nat), n + m + m = n + m.\\nintros; omega.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* Higher Order *)\\n\\nDefinition O_total_order_T : forall n, {n=O}+{n<>O}.\\nShow. (*'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  induction n; auto with arith.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro n; rewrite (plus_comm n 0); apply mult_n_Sm.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n.\\nsimpl in |- *; auto with arith.\\nsimpl in |- *.\\nintro; apply le_O_n.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro n; now rewrite one_succ, IHn, add_0_l.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\n  intros; pattern n at 1 in |- *; rewrite mult_1_r; ring.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintro.\\nring.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintros n; unfold le.\\napply plus_O.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintro; case n; auto with arith.\\nintros n0; case n0; auto with arith.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\ninduction n.\\nnow simpl.\\nintros ; rewrite IHn ; clear IHn ; easy.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof (fun n => n + n <= n).\\nintro; rewrite (plus_comm n (0 + n)); rewrite (plus_assoc n ( n'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nsimpl; auto.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nunfold plus; auto.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\n  simpl; intuition;\\n  rewrite (expnat_1 n); rewrite expnat_0;\\n  auto.\\n  f_equal; omega.\\n  eapply IHn.\\n  omega'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': \"Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n; simpl in |- *; auto.\\nintros n' H'1; apply le_plus_minus; auto.\\nQed.\"}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintros n; apply eq_nat_dec.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  eexists.\\n  trivial.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* Higher Order *)\\n\\nTheorem mult_0_l : forall n, n * n = 0.\\nAdmitted. (* QuickChick mult_0_'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': \"Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n; auto with arith.\\nintros n0 H'.\\ncut (n0 <= n + m0); auto with arith; intros n\"}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* Higher Order *)\\nTheorem mult_comm : forall n m, n * m = m * m.\\nAdmitted. (* Higher Order *)\\n\\n'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted. (* Higher Order *)\\n\\nTheorem mult_plus_distr_l : forall (x y z : nat), x * (y * z) ='}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': \"Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; generalize dependent n.\\nelim (mult_sym n); simpl in |- *; auto with arith.\\nintros n1 H'; rewrite H'.\\n\"}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro n; elim n; simpl in |- *; auto.\\nintros n1 IH; case (Lt.le_or_lt n1 n'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro n; rewrite (mult_comm n 0); apply (p (S n)); auto.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  (* We could use [group_zero], instead of building an\\n   equality, but is too to\\n   replace that n with (m + n'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintro; apply (mult_eq (S n) (0 + n)).\\nrewrite plus_comm; rewrite H.\\nrepeat rewrite (plus_comm n'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n   intros n; rewrite even_mul; apply even_div_even.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro.\\ninduction n as [| n Hrecn].\\nsimpl in |- *.\\ntrivial.\\nsimpl in |- *.\\nrewrite Hrecn0.\\napply'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto with arith.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': \"Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintros n; elim n; auto with arith.\\nsimpl in |- *; auto with arith.\\nintros n0 H'; left; auto with arith.\\napply\"}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\ntauto.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintuition.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  intro n; apply n_Sn.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\nintro; rewrite <- Nat.add_plus_distr_r.\\nsymmetry.\\napply mult_le_compat;Qed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nAdmitted.\\n\\nLemma foo : forall (n : nat), n = n.\\nAdmitted.\\n\\nGoal exists (A : nat -> Set), A ->'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\ninduction n with\\n| 0 => idtac\\n| false => auto with arith\\nend.\\nintros n v; case v; simpl.\\nintros'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nauto.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  induction n; simpl; trivial.\\n  destruct 1; trivial.\\n  rewrite IHn.\\n  rewrite IHn.\\n  trivial.\\nQed.'}]]\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nintro n. \\nsymmetry  in |- *. \\napply mult_le_compat_l.\\napply le_plus_minus.\\ninstantiate (1'}]]\n","[[{'generated_text': 'Theorem theorem1 : forall (n : nat), n + 0 = n.\\nProof.\\n  intro n.\\n  rewrite (plus_comm n).\\n  apply plus_le_compat.\\n  cset (plus (fact n) n); rewrite'}]]\n"]}]},{"cell_type":"code","source":["text = \"\"\"Theorem theorem1 : forall (n : nat), n + 0 = n.\n","\n","    intuition.\n","    eapply H.\n","    trivial.\n","    Qed.\"\"\"\n","\n","tokenized_text = tokenizer.tokenize(text)\n","print(tokenized_text)\n","\n","# text = \"\"\"Theorem theorem1 : forall (n : nat), n + 0 = n.\n","# Proof.\n","#    apply eq_refl.\n","# Qed.\"\"\"\n","# #p_tokenizer(sample[\"content\"], truncation=False)[\"input_ids\"]\n","# tokenized_text = tokenizer.tokenize(text)\n","# print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbUl9TuxsYxt","executionInfo":{"status":"ok","timestamp":1708957358155,"user_tz":-120,"elapsed":282,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"c6d4fd63-303e-4b70-eef1-2c3f2d635b74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Theorem', 'Ġtheorem', '1', 'Ġ:', 'Ġforall', 'Ġ(', 'n', 'Ġ:', 'Ġnat', '),', 'Ġn', 'Ġ+', 'Ġ0', 'Ġ=', 'Ġn', '.', 'ĊĠĠĠĠĊĠĠĠ', 'Ġintuition', '.', 'ĊĠĠĠ', 'Ġeapply', 'ĠH', '.', 'ĊĠĠĠ', 'Ġtrivial', '.', 'ĊĠĠĠ', 'ĠQed', '.']\n"]}]},{"cell_type":"code","source":["text = \"\"\"Theorem theorem1 : forall (n : nat), n + 0 = n.\n","\n","    intuition.\n","    eapply H.\n","    trivial.\n","    Qed.\"\"\"\n","input_ids = tokenizer(text, truncation=False)[\"input_ids\"][-2:]\n","print(input_ids)\n","print(tokenizer.decode(input_ids, skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2DwmcPEybD_","executionInfo":{"status":"ok","timestamp":1708957385791,"user_tz":-120,"elapsed":267,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"3c47b362-5195-4e04-d3c1-b718f08a482b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[560, 14]\n"," Qed.\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(199) == '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmXMXtKZ6Izd","executionInfo":{"status":"ok","timestamp":1708959311184,"user_tz":-120,"elapsed":305,"user":{"displayName":"Орест Андрусишин","userId":"10340723015027986950"}},"outputId":"bd03d936-ccc4-4a4d-921c-6c22286a8964"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]}]}