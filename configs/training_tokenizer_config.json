{
    "vocab_size"                   : 30000,
    "base_model"                   : "gpt2",
    "batch_size"                   : 20,

    "raw_train_json"               : "./datasets/dataset_train.json",

    "push_to_hub"                  : false,
    "tokenizer_repo_name"          : "Andrusyshyn/gpt2-coq-tokenizer",
    "tokenizer_output_dir"         : "gpt2-coq-tokenizer_local",
    "run_name"                     : "experimental",



    "drive_mounted"                : false,
    "drive_mounted_path"           : "/content/gdrive/",
    "train_data_archived"          : true,
    "raw_train_archive"            : "./dataset_train.zip",
    "user_email"                   : "user_email",
    "user_name"                    : "user_name"
}