{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlOfuAT4e-xu"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "from huggingface_hub import notebook_login, Repository\n",
        "from datasets import load_dataset, Dataset\n",
        "from accelerate import Accelerator\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
        "\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "from google.colab import runtime"
      ],
      "metadata": {
        "id": "J6UDRGWQg4UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING\n"
      ],
      "metadata": {
        "id": "iy7X6uix8Odn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HYPER PARAMS, GLOBAL VARS, FUNCTIONS"
      ],
      "metadata": {
        "id": "PbZLtjmyh0Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPER PARAMETERS\n",
        "context_length                 = 1024\n",
        "train_batch_size               = 8\n",
        "eval_batch_size                = 8\n",
        "weight_decay                   = 0.1\n",
        "lr                             = 8e-4\n",
        "lr_scheduler_func              = get_cosine_schedule_with_warmup\n",
        "adamw_b1                       = 0.9\n",
        "adamw_b2                       = 0.95\n",
        "adamw_e                        = 1e-8\n",
        "num_warmup_steps               = 30\n",
        "gradient_accumulation_steps    = 4\n",
        "gradient_checkpointing         = False\n",
        "eval_steps                     = 100\n",
        "num_train_epochs               = 10\n",
        "mixed_precision                = \"fp16\"\n",
        "\n",
        "\n",
        "#GLOBAL VARS\n",
        "have_git_write_access          = False                    # set to True if you need to commit changes\n",
        "user_email                     = \"user_email\"             # only needed if above have_git_write_access == True\n",
        "user_name                      = \"user_name\"              # only needed if above have_git_write_access == True\n",
        "\n",
        "drive_mounted                  = False\n",
        "drive_mounted_path             = '/content/gdrive/'       # only needed if drive_mounted == True\n",
        "save_json_logs                 = True                     # set to True to save training results into json file (drive_train_res_path)\n",
        "drive_train_res_path           = './test_run.json'        # only needed if save_json_logs == True\n",
        "save_tensorboard_logs          = True                     # set to True to save training results into tensorboard run (tensorboard_run_path)\n",
        "tensorboard_run_path           = './test_run'             # only needed if save_tensorboard_logs == True\n",
        "\n",
        "config_file                    = None\n",
        "\n",
        "data_archived                  = True                     # set to True if you need to unarchive the data\n",
        "raw_train_archive              = \"./dataset_train.zip\"    # only needed if data_archived == True\n",
        "raw_valid_archive              = \"./dataset_valid.zip\"    # only needed if data_archived == True\n",
        "raw_train_json                 = \"./dataset_train.json\"   # if data_archived == True, json file will be extracted into the current working directory\n",
        "raw_valid_json                 = \"./dataset_valid.json\"   # if data_archived == True, json file will be extracted into the current working directory\n",
        "\n",
        "tokenizer_repo_name            = \"Andrusyshyn/gpt2-coq-tokenizer\"\n",
        "tokenizer_commit_hash          = \"0e1383183b23c6764d83c88b83fa99de2a297199\"\n",
        "\n",
        "init_model                     = \"gpt2\"                   # base model Hugging Face name\n",
        "n_layer                        = 6\n",
        "n_head                         = 6\n",
        "n_embd                         = 384\n",
        "model_repo_name                = \"Andrusyshyn/gpt2-pretrained-for-coq-pt-custom-train\"\n",
        "model_output_dir               = \"./gpt2-pretrained-for-coq-pt-custom-train-local\"        # local dir to save the model\n",
        "\n",
        "push_to_hub                    = False                     # set to True to push the model\n",
        "run_name                       = \"test_branch\"             # branch name (only needed if push_to_hub == True)\n",
        "\n",
        "partially_trained              = False                     # set to True to continue model training from specific commit hash (model_commit_hash)\n",
        "model_commit_hash              = \"\"                        # only needed if partially_trained == True\n",
        "previously_completed_steps     = 0                         # only needed if partially_trained == True\n",
        "previous_global_steps          = 0                         # only needed if partially_trained == True\n",
        "previous_step                  = 0                         # only needed if partially_trained == True\n",
        "stopped_epoch                  = 0                         # only needed if partially_trained == True\n",
        "\n",
        "torch_seed                     = 7\n",
        "data_seed                      = 23"
      ],
      "metadata": {
        "id": "7VRZhQTKZaSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_config(config_file: str):\n",
        "    \"\"\"\n",
        "    Parses config_file and sets global variables.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config_file : str\n",
        "        path to config file.\n",
        "    \"\"\"\n",
        "    global context_length, train_batch_size, eval_batch_size, weight_decay, lr,\\\n",
        "           lr_scheduler_func, adamw_b1, adamw_b2, adamw_e, num_warmup_steps, gradient_accumulation_steps,\\\n",
        "           gradient_checkpointing, eval_steps, num_train_epochs, mixed_precision, save_json_logs,\\\n",
        "           train_logs_path, save_tensorboard_logs, tensorboard_run_path, raw_train_json, raw_valid_json,\\\n",
        "           tokenizer_repo_name, tokenizer_commit_hash, init_model, n_layer, n_head, n_embd, model_repo_name,\\\n",
        "           model_repo_name, model_output_dir, push_to_hub, run_name, partially_trained, model_commit_hash,\\\n",
        "           previously_completed_steps, previous_global_steps, previous_step, stopped_epoch, torch_seed, data_seed,\\\n",
        "           have_git_write_access, user_email, user_name, drive_mounted, drive_mounted_path, data_archived, raw_train_archive, raw_valid_archive\n",
        "    with open(config_file, mode='r') as conf_file:\n",
        "        conf_data = json.load(conf_file)\n",
        "\n",
        "    context_length                 = conf_data[\"context_length\"]\n",
        "    train_batch_size               = conf_data[\"train_batch_size\"]\n",
        "    eval_batch_size                = conf_data[\"eval_batch_size\"]\n",
        "    weight_decay                   = conf_data[\"weight_decay\"]\n",
        "    lr                             = conf_data[\"lr\"]\n",
        "    if conf_data[\"lr_scheduler_func\"] == \"cosine\":\n",
        "        lr_scheduler_func = get_cosine_schedule_with_warmup\n",
        "    elif conf_data[\"lr_scheduler_func\"] == \"linear\":\n",
        "        lr_scheduler_func = get_linear_schedule_with_warmup\n",
        "    elif conf_data[\"lr_scheduler_func\"] == \"const\":\n",
        "        lr_scheduler_func = get_constant_schedule_with_warmup\n",
        "    else:\n",
        "        lr_scheduler_func = get_cosine_schedule_with_warmup\n",
        "    adamw_b1                       = conf_data[\"adamw_b1\"]\n",
        "    adamw_b2                       = conf_data[\"adamw_b2\"]\n",
        "    adamw_e                        = conf_data[\"adamw_e\"]\n",
        "    num_warmup_steps               = conf_data[\"num_warmup_steps\"]\n",
        "    gradient_accumulation_steps    = conf_data[\"gradient_accumulation_steps\"]\n",
        "    gradient_checkpointing         = conf_data[\"gradient_checkpointing\"]\n",
        "    eval_steps                     = conf_data[\"eval_steps\"]\n",
        "    num_train_epochs               = conf_data[\"num_train_epochs\"]\n",
        "    mixed_precision                = conf_data[\"mixed_precision\"]\n",
        "\n",
        "\n",
        "    save_json_logs                 = conf_data[\"save_json_logs\"]\n",
        "    drive_train_res_path           = conf_data[\"train_logs_path\"]\n",
        "    save_tensorboard_logs          = conf_data[\"save_tensorboard_logs\"]\n",
        "    tensorboard_run_path           = conf_data[\"tensorboard_run_path\"]\n",
        "\n",
        "    raw_train_json                 = conf_data[\"raw_train_json\"]\n",
        "    raw_valid_json                 = conf_data[\"raw_valid_json\"]\n",
        "\n",
        "    tokenizer_repo_name            = conf_data[\"tokenizer_repo_name\"]\n",
        "    tokenizer_commit_hash          = conf_data[\"tokenizer_commit_hash\"]\n",
        "\n",
        "    init_model                     = conf_data[\"init_model\"]\n",
        "    n_layer                        = conf_data[\"n_layer\"]\n",
        "    n_head                         = conf_data[\"n_head\"]\n",
        "    n_embd                         = conf_data[\"n_embd\"]\n",
        "    model_repo_name                = conf_data[\"model_repo_name\"]\n",
        "    model_output_dir               = conf_data[\"model_output_dir\"]\n",
        "\n",
        "    push_to_hub                    = conf_data[\"push_to_hub\"]\n",
        "    run_name                       = conf_data[\"run_name\"]\n",
        "\n",
        "    partially_trained              = conf_data[\"partially_trained\"]\n",
        "    model_commit_hash              = conf_data[\"model_commit_hash\"]\n",
        "    previously_completed_steps     = conf_data[\"previously_completed_steps\"]\n",
        "    previous_global_steps          = conf_data[\"previous_global_steps\"]\n",
        "    previous_step                  = conf_data[\"previous_step\"]\n",
        "    stopped_epoch                  = conf_data[\"stopped_epoch\"]\n",
        "\n",
        "    torch_seed                     = conf_data[\"torch_seed\"]\n",
        "    data_seed                      = conf_data[\"data_seed\"]\n",
        "\n",
        "    # Collab only vars:\n",
        "    have_git_write_access          = conf_data[\"have_git_write_access\"]\n",
        "    user_email                     = conf_data[\"user_email\"]\n",
        "    user_name                      = conf_data[\"user_name\"]\n",
        "    drive_mounted                  = conf_data[\"drive_mounted\"]\n",
        "    drive_mounted_path             = conf_data[\"drive_mounted_path\"]\n",
        "    data_archived                  = conf_data[\"data_archived\"]\n",
        "    raw_train_archive              = conf_data[\"raw_train_archive\"]\n",
        "    raw_valid_archive              = conf_data[\"raw_valid_archive\"]"
      ],
      "metadata": {
        "id": "FnwJ-cDtE8bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(inputs: torch.Tensor, logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculates mean CrossEntropyLoss across samples in the batch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    inputs : torch.Tensor\n",
        "        tensor of input sequences. Dimensions: batch_size X context_length.\n",
        "    logits: torch.\n",
        "        logits outputted by model. Dimensions: batch_size X context_length X vocab_size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        mean CrossEntropyLoss loss across samples in the batch.\n",
        "    \"\"\"\n",
        "    # inputs [batch_size X cl]\n",
        "    # logits [btach_size X cl X vocab_size]\n",
        "    # Our labels start from second sequence token because first one does not have preceding token.\n",
        "    # We drop last logit because last sequence token does not have subsequent token, so no label to compare\n",
        "    shifted_labels = inputs[..., 1:].contiguous()\n",
        "    shifted_logits = logits[..., :-1, :].contiguous()\n",
        "\n",
        "    loss_func = CrossEntropyLoss(reduction='none')\n",
        "    # loss [batch_size * (cl-1)] = loss_fct([batch_size * (cl-1) X vocab_size], [batch_size * (cl-1)])\n",
        "    loss = loss_func(shifted_logits.view(-1, shifted_logits.size(-1)), shifted_labels.view(-1))\n",
        "    # loss_per_sequence [batch_size]\n",
        "    loss_per_sequence = loss.view(shifted_logits.size(0), shifted_logits.size(1)).mean(axis=1)\n",
        "    return loss_per_sequence.mean()"
      ],
      "metadata": {
        "id": "dVnxDLITeUwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wd_parameters(model: GPT2LMHeadModel, no_decay:list[str]=[\"bias\", r\"ln_.{1,2}\\.weight\"]) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Returns parameters with and without weight decay.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : GPT2LMHeadModel\n",
        "        model\n",
        "    no_decay : list[str], optional\n",
        "        list of subwords to look for in the model parameters names. This\n",
        "        parameters will have no decay.\n",
        "        Default value is [\"bias\", r\"ln_.{1,2}\\.weight\"].\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[dict]\n",
        "        {\"params\": wd_params,  \"weight_decay\": weight_decay},\n",
        "        {\"params\": nwd_params, \"weight_decay\": 0.0},\n",
        "        wd_params and nwd_params are of type Parameter.\n",
        "    \"\"\"\n",
        "    wd_params = []\n",
        "    nwd_params = []\n",
        "    for name, params in model.named_parameters():\n",
        "        if any(re.search(nd_reg, name) for nd_reg in no_decay):\n",
        "            nwd_params.append(params)\n",
        "        else:\n",
        "            wd_params.append(params)\n",
        "    return [\n",
        "        {\"params\": wd_params,  \"weight_decay\": weight_decay},\n",
        "        {\"params\": nwd_params, \"weight_decay\": 0.0},\n",
        "    ]"
      ],
      "metadata": {
        "id": "s4G7PyR8eaZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenized_dataset(p_raw_dataset: Dataset, p_context_length: int, p_tokenizer: AutoTokenizer) -> Dataset:\n",
        "    \"\"\"\n",
        "    Tokenizes raw dataset p_raw_dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    p_raw_dataset : Dataset\n",
        "        raw dataset ot tokenize\n",
        "    p_context_length : int\n",
        "        context length\n",
        "    p_tokenizer : AutoTokenizer\n",
        "        tokenizer\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dataset\n",
        "        tokenized dataset, each entry is the input sequence of the\n",
        "        context_length length.\n",
        "    \"\"\"\n",
        "    concatenated_tokenized_samples = []\n",
        "    for sample in p_raw_dataset:\n",
        "        tokenized_sample = p_tokenizer(sample[\"content\"], truncation=False)[\"input_ids\"]\n",
        "        concatenated_tokenized_samples.extend(tokenized_sample + [p_tokenizer.eos_token_id])\n",
        "\n",
        "    tokenized_dataset_list = []\n",
        "    for i in range(0, len(concatenated_tokenized_samples), p_context_length):\n",
        "        input_ids = concatenated_tokenized_samples[i : i + p_context_length]\n",
        "        if len(input_ids) == p_context_length:\n",
        "            tokenized_dataset_list.append(torch.tensor(input_ids))\n",
        "\n",
        "    return Dataset.from_dict({\"input_ids\": tokenized_dataset_list})"
      ],
      "metadata": {
        "id": "ZHTy8qSxLjjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(filepath: str, split: str, results: list):\n",
        "    \"\"\"\n",
        "    Save training logs to the filepath. Extends current logs in the filepath.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        path to the output JSON file with the following structure:\n",
        "        {\n",
        "            \"hyperparams\": {},\n",
        "            \"train\": [\n",
        "                results1,\n",
        "                results2,\n",
        "                ...\n",
        "            ],\n",
        "            \"valid\": [\n",
        "                results1,\n",
        "                results2,\n",
        "                ...\n",
        "            ]\n",
        "        }\n",
        "    split : str\n",
        "        \"train\" or \"valid\"\n",
        "    results : list\n",
        "        list of training logs.\n",
        "    \"\"\"\n",
        "    if not save_json_logs:\n",
        "        return\n",
        "    if split not in {\"train\", \"valid\"}:\n",
        "        print(\"ERROR: INVALID SPLIT\")\n",
        "        return\n",
        "    _run_name = run_name\n",
        "    if not push_to_hub:\n",
        "        _run_name = \"\"\n",
        "    _tensorboard_run_path = tensorboard_run_path\n",
        "    if not save_tensorboard_logs:\n",
        "        _tensorboard_run_path = \"\"\n",
        "    _lr_scheduler_type = \"cosine\"\n",
        "    if lr_scheduler_func == get_linear_schedule_with_warmup:\n",
        "        _lr_scheduler_type = \"linear\"\n",
        "    elif lr_scheduler_func == get_constant_schedule_with_warmup:\n",
        "        _lr_scheduler_type = \"const\"\n",
        "    hyperparams_dict = {\n",
        "        \"context_length\"                 : context_length,\n",
        "        \"train_batch_size\"               : train_batch_size,\n",
        "        \"eval_batch_size\"                : eval_batch_size,\n",
        "        \"weight_decay\"                   : weight_decay,\n",
        "        \"lr\"                             : lr,\n",
        "        \"lr_scheduler_type\"              : _lr_scheduler_type,\n",
        "        \"adamw_b1\"                       : adamw_b1,\n",
        "        \"adamw_b2\"                       : adamw_b2,\n",
        "        \"adamw_e\"                        : adamw_e,\n",
        "        \"num_warmup_steps\"               : num_warmup_steps,\n",
        "        \"gradient_accumulation_steps\"    : gradient_accumulation_steps,\n",
        "        \"gradient_checkpointing\"         : gradient_checkpointing,\n",
        "        \"eval_steps\"                     : eval_steps,\n",
        "        \"num_train_epochs\"               : num_train_epochs,\n",
        "        \"mixed_precision\"                : mixed_precision,\n",
        "        \"tokenizer_repo_name\"            : tokenizer_repo_name,\n",
        "        \"tokenizer_commit_hash\"          : tokenizer_commit_hash,\n",
        "        \"init_model\"                     : init_model,\n",
        "        \"n_layer\"                        : n_layer,\n",
        "        \"n_head\"                         : n_head,\n",
        "        \"n_embd\"                         : n_embd,\n",
        "        \"model_repo_name\"                : model_repo_name,\n",
        "        \"run_name\"                       : _run_name,\n",
        "        \"drive_train_res_path\"           : drive_train_res_path,\n",
        "        \"tensorboard_run_path\"           : _tensorboard_run_path,\n",
        "        \"torch_seed\"                     : torch_seed,\n",
        "        \"data_seed\"                      : data_seed\n",
        "    }\n",
        "    json_data = {\"train\": [], \"valid\": []}\n",
        "    if os.path.exists(filepath):\n",
        "        with open(filepath, 'r') as json_file:\n",
        "            json_data = json.load(json_file)\n",
        "    json_data[\"hyperparams\"] = hyperparams_dict\n",
        "    json_data[split].extend(results)\n",
        "    with open(filepath, 'w') as json_file:\n",
        "        json.dump(json_data, json_file, indent=4)"
      ],
      "metadata": {
        "id": "3dJLZFnz2StA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_tensorboard(json_filepath: str, tensorboard_run_path: str):\n",
        "    \"\"\"\n",
        "    Adds training logs from json_filepath to the tensorboard tensorboard_run_path.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    json_filepath : str\n",
        "        path to the JSON file with logs\n",
        "    tensorboard_run_path : str\n",
        "        path to the output tensorboard run\n",
        "    \"\"\"\n",
        "    if not save_tensorboard_logs:\n",
        "        return\n",
        "    if not os.path.exists(json_filepath):\n",
        "        print(\"ERROR: json_filepath DOES NOT EXIST\")\n",
        "        return\n",
        "    with open(json_filepath, mode=\"r\") as json_file:\n",
        "        json_data = json.load(json_file)\n",
        "        one_step_tokens = json_data[\"hyperparams\"][\"train_batch_size\"] * json_data[\"hyperparams\"][\"gradient_accumulation_steps\"] * json_data[\"hyperparams\"][\"context_length\"]\n",
        "        writer = SummaryWriter(tensorboard_run_path)\n",
        "        prev_comleted_steps = json_data[\"train\"][0][\"completed_steps\"]\n",
        "        prev_lr = json_data[\"train\"][0][\"lr\"][0]\n",
        "        train_losses = []\n",
        "        cs = 0\n",
        "        for entry in json_data[\"train\"]:\n",
        "            cs = entry[\"completed_steps\"]\n",
        "            if cs == prev_comleted_steps:\n",
        "                train_losses.append(entry[\"loss/train\"])\n",
        "                continue\n",
        "            else:\n",
        "                writer.add_scalar(\"Loss/Train\", sum(train_losses)/len(train_losses), prev_comleted_steps * one_step_tokens)\n",
        "                writer.add_scalar(\"Learning Rate\", prev_lr, prev_comleted_steps * one_step_tokens)\n",
        "                train_losses = [entry[\"loss/train\"]]\n",
        "                prev_comleted_steps = cs\n",
        "                prev_lr = entry[\"lr\"][0]\n",
        "        writer.add_scalar(\"Loss/Train\", sum(train_losses)/len(train_losses), cs * one_step_tokens)\n",
        "        writer.add_scalar(\"Learning Rate\", prev_lr, cs * one_step_tokens)\n",
        "\n",
        "        for entry in json_data[\"valid\"]:\n",
        "            cs = entry[\"completed_steps\"]\n",
        "            writer.add_scalar(\"Loss/Eval\", entry[\"loss/eval\"], cs * one_step_tokens)\n",
        "            writer.add_scalar(\"Perplexity/Eval\", entry[\"perplexity\"], cs * one_step_tokens)\n",
        "        writer.close()"
      ],
      "metadata": {
        "id": "negAfxKOtdOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CONFIGURING"
      ],
      "metadata": {
        "id": "1avaSDQZ_-M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MOUNTING DRIVE FOR SAVING LOGS\n",
        "if drive_mounted:\n",
        "    drive.mount(drive_mounted_path)"
      ],
      "metadata": {
        "id": "0mp6KpOhAC2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config_file is not None:\n",
        "    parse_config(config_file)\n",
        "data_generator = torch.Generator().manual_seed(data_seed)\n",
        "torch.manual_seed(torch_seed)"
      ],
      "metadata": {
        "id": "D13IZC53F1Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURING GIT CREDENTIALS\n",
        "if have_git_write_access:\n",
        "    !git config --global user.email \"{user_email}\"\n",
        "    !git config --global user.name \"{user_name}\"\n",
        "\n",
        "# To set Hugging Face token (for writing access) create HF_TOKEN secret in Google Collab or use notebook_login()"
      ],
      "metadata": {
        "id": "5Z8pOm7N_hql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURING GIT DIRECTORIES\n",
        "if push_to_hub:\n",
        "    repo = Repository(model_output_dir, clone_from=model_repo_name)\n",
        "    repo.git_checkout(run_name, create_branch_ok=True)"
      ],
      "metadata": {
        "id": "xxYRTff8gOzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DATASETS"
      ],
      "metadata": {
        "id": "QCzfmzzIgSXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNPACK DATASETS\n",
        "if data_archived:\n",
        "    if raw_train_archive.endswith(\".gz\"):\n",
        "        !gzip -dkv \"{raw_train_archive}\"\n",
        "    if raw_train_archive.endswith(\".zip\"):\n",
        "        !unzip \"{raw_train_archive}\"\n",
        "    if raw_valid_archive.endswith(\".gz\"):\n",
        "        !gzip -dkv \"{raw_valid_archive}\"\n",
        "    if raw_valid_archive.endswith(\".zip\"):\n",
        "        !unzip \"{raw_valid_archive}\""
      ],
      "metadata": {
        "id": "DyIeffM-P9if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATASETS\n",
        "data_files = {\"train\": raw_train_json, \"validation\": raw_valid_json}\n",
        "raw_datasets = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
        "print(raw_datasets)"
      ],
      "metadata": {
        "id": "CtwDIj4TP-NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING TOKENIZER\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_name, revision=tokenizer_commit_hash)\n",
        "print(\"tokenizer vocab size: \", len(tokenizer))\n",
        "# TOKENIZE RAW DATASETS\n",
        "train_dataset = get_tokenized_dataset(raw_datasets[\"train\"],      context_length, tokenizer)\n",
        "valid_dataset = get_tokenized_dataset(raw_datasets[\"validation\"], context_length, tokenizer)\n",
        "train_dataset.set_format(\"torch\")\n",
        "valid_dataset.set_format(\"torch\")\n",
        "\n",
        "# CREATE DATALOADERS\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, generator=data_generator)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=eval_batch_size)\n",
        "print(train_dataset)\n",
        "print(valid_dataset)\n",
        "print(\"len(train_dataloader): \", len(train_dataloader))\n",
        "print(\"len(valid_dataloader): \", len(valid_dataloader))"
      ],
      "metadata": {
        "id": "EBqU_TnKQBiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CONFIGURING MODEL AND TRAINING"
      ],
      "metadata": {
        "id": "uozaVBJNiCFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURING MODEL\n",
        "model = None\n",
        "if partially_trained:\n",
        "    print(\"Loading partially trained model\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_repo_name, revision=model_commit_hash)\n",
        "else:\n",
        "    print(\"Training from scratch\")\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        init_model,\n",
        "        vocab_size=len(tokenizer),\n",
        "        n_ctx=context_length,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        n_layer = n_layer,\n",
        "        n_head = n_head,\n",
        "        n_embd = n_embd\n",
        "    )\n",
        "    model = GPT2LMHeadModel(config)\n",
        "print()\n",
        "if gradient_checkpointing:\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "optimizer = AdamW(get_wd_parameters(model), lr=lr, betas=(adamw_b1, adamw_b2), eps=adamw_e)\n",
        "accelerator = Accelerator(mixed_precision=mixed_precision)\n",
        "model, optimizer, train_dataloader, valid_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, valid_dataloader\n",
        ")\n",
        "\n",
        "num_steps_per_epoch = len(train_dataloader)\n",
        "print(\"Num steps per epoch: \", num_steps_per_epoch)\n",
        "num_training_completed_steps = (num_train_epochs * num_steps_per_epoch) // gradient_accumulation_steps\n",
        "if ((num_train_epochs * num_steps_per_epoch) % gradient_accumulation_steps != 0):\n",
        "    num_training_completed_steps += 1\n",
        "print(\"Num optimizer steps: \", num_training_completed_steps)\n",
        "\n",
        "if lr_scheduler_func == get_constant_schedule_with_warmup:\n",
        "    lr_scheduler = lr_scheduler_func(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=num_warmup_steps\n",
        "    )\n",
        "else:\n",
        "    lr_scheduler = lr_scheduler_func(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_completed_steps\n",
        "    )\n",
        "\n",
        "\n",
        "if partially_trained:\n",
        "    with torch.no_grad():\n",
        "        for ind in range(previously_completed_steps):\n",
        "            if ((ind < num_warmup_steps) or (lr_scheduler.get_lr()[0] > (0.1 * lr))):\n",
        "                lr_scheduler.step()\n",
        "\n",
        "print()\n",
        "print(f\"Model size:                                        {model.num_parameters()}\")\n",
        "print(f\"Model size (only trainable params):                {model.num_parameters(only_trainable=True)}\")\n",
        "print(f\"Model size (only trainable non-embeddings params): {model.num_parameters(only_trainable=True, exclude_embeddings=True)}\")"
      ],
      "metadata": {
        "id": "nxQPSwfzN7IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate() -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Calculates validation loss and perplexity of the model on the validation dataset from valid_dataloader.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[float, float]\n",
        "        validation loss, validation perplexity\n",
        "    \"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(valid_dataloader):\n",
        "            with torch.no_grad():\n",
        "                logits = model(batch[\"input_ids\"]).logits\n",
        "                loss = loss_function(batch[\"input_ids\"], logits)\n",
        "                losses.append(loss.item())\n",
        "    loss = torch.mean(torch.Tensor(losses))\n",
        "    try:\n",
        "        perplexity = torch.exp(loss)\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss.item(), perplexity.item()"
      ],
      "metadata": {
        "id": "hK4sg5C3eybv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING LOOP\n",
        "log_buffer = []\n",
        "\n",
        "model.train()\n",
        "completed_steps = 0\n",
        "global_steps = 0\n",
        "if partially_trained:\n",
        "    completed_steps = previously_completed_steps\n",
        "    global_steps = previous_global_steps\n",
        "for epoch in range(num_train_epochs):\n",
        "    for step, batch in tqdm(\n",
        "        enumerate(train_dataloader, start=0), total=num_steps_per_epoch\n",
        "    ):\n",
        "        if partially_trained and ((epoch<stopped_epoch) or ((epoch==stopped_epoch) and (step <= previous_step))):\n",
        "            continue\n",
        "\n",
        "        logits = model(batch[\"input_ids\"]).logits\n",
        "        loss = loss_function(batch[\"input_ids\"], logits)\n",
        "################################################################################\n",
        "        log_train = {\n",
        "                \"loss/train\": loss.item(),\n",
        "                \"completed_steps\": completed_steps,\n",
        "                \"lr\": lr_scheduler.get_lr(),\n",
        "                \"global_steps\" : global_steps,\n",
        "                \"epoch\": epoch,\n",
        "                \"steps\": step,\n",
        "        }\n",
        "        if ((completed_steps % 10 == 0) and (global_steps % gradient_accumulation_steps == 0)):\n",
        "            accelerator.print(log_train)\n",
        "        if save_json_logs:\n",
        "            log_buffer.append(log_train)\n",
        "################################################################################\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        accelerator.backward(loss)\n",
        "        global_steps += 1\n",
        "################################################################################\n",
        "        if global_steps % gradient_accumulation_steps == 0:\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            if ((completed_steps < num_warmup_steps) or (lr_scheduler.get_lr()[0] > (0.1 * lr))):\n",
        "                lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            completed_steps += 1\n",
        "        if (global_steps % (eval_steps * gradient_accumulation_steps)) == 0:\n",
        "            if save_json_logs:\n",
        "                save_results(drive_train_res_path, \"train\", log_buffer)\n",
        "                log_buffer = []\n",
        "            eval_loss, perplexity = evaluate()\n",
        "            log_eval = {\n",
        "                    \"loss/eval\": eval_loss,\n",
        "                    \"perplexity\": perplexity,\n",
        "                    \"completed_steps\": completed_steps,\n",
        "                    \"lr\": lr_scheduler.get_lr(),\n",
        "                    \"global_steps\" : global_steps,\n",
        "                    \"epoch\": epoch,\n",
        "                    \"steps\": step,\n",
        "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
        "            }\n",
        "            accelerator.print(log_eval)\n",
        "            save_results(drive_train_res_path, \"valid\", [log_eval])\n",
        "            accelerator.wait_for_everyone()\n",
        "            unwrapped_model = accelerator.unwrap_model(model)\n",
        "            unwrapped_model.save_pretrained(model_output_dir, save_function=accelerator.save)\n",
        "            tokenizer.save_pretrained(model_output_dir)\n",
        "            if accelerator.is_main_process:\n",
        "                if push_to_hub:\n",
        "                    repo.push_to_hub(\n",
        "                        commit_message=f\"Training in progress: completed_steps {completed_steps}; global_steps {global_steps};\\\n",
        "                                         epoch {epoch}; steps {step}; lr {lr_scheduler.get_lr()};\\\n",
        "                                         loss/eval {eval_loss}; perplexity {perplexity}; loss/train {loss.item() * gradient_accumulation_steps}\",\n",
        "                        blocking=False\n",
        "                    )\n",
        "            model.train()\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "#GRADIENT UPDATE (In case (global_steps % gradient_accumulation_steps != 0)\n",
        "last_eval_log_train_loss = 0\n",
        "if (global_steps % gradient_accumulation_steps != 0):\n",
        "    for step, batch in tqdm(\n",
        "        enumerate(train_dataloader, start=0), total=(gradient_accumulation_steps - (global_steps % gradient_accumulation_steps)) - 1    # -1 here is purely for better visualisation of tqdm progress bar\n",
        "    ):\n",
        "        logits = model(batch[\"input_ids\"]).logits\n",
        "        loss = loss_function(batch[\"input_ids\"], logits)\n",
        "        last_eval_log_train_loss = loss.item()\n",
        "        log_train = {\n",
        "                \"loss/train\": loss.item(),\n",
        "                \"completed_steps\": completed_steps,\n",
        "                \"lr\": lr_scheduler.get_lr(),\n",
        "                \"global_steps\" : global_steps,\n",
        "                \"epoch\": epoch,\n",
        "                \"steps\": step + num_steps_per_epoch,\n",
        "        }\n",
        "        if save_json_logs:\n",
        "            log_buffer.append(log_train)\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        accelerator.backward(loss)\n",
        "        global_steps += 1\n",
        "        if global_steps % gradient_accumulation_steps == 0:\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            if ((completed_steps < num_warmup_steps) or (lr_scheduler.get_lr()[0] > (0.1 * lr))):\n",
        "                lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            completed_steps += 1\n",
        "            break\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "# FINAL EVALUATE AND SAVE\n",
        "additional_steps = 0\n",
        "if (global_steps % gradient_accumulation_steps != 0):\n",
        "    additional_steps = gradient_accumulation_steps - (global_steps % gradient_accumulation_steps)\n",
        "with torch.no_grad():\n",
        "    last_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        logits = model(batch[\"input_ids\"]).logits\n",
        "        loss = loss_function(batch[\"input_ids\"], logits)\n",
        "        last_train_loss = loss.item()\n",
        "        break\n",
        "log_train = {\n",
        "        \"loss/train\": last_train_loss,\n",
        "        \"completed_steps\": completed_steps,\n",
        "        \"lr\": lr_scheduler.get_lr(),\n",
        "        \"global_steps\" : global_steps,\n",
        "        \"epoch\": epoch,\n",
        "        \"steps\": num_steps_per_epoch + additional_steps,\n",
        "}\n",
        "if save_json_logs:\n",
        "    log_buffer.append(log_train)\n",
        "    save_results(drive_train_res_path, \"train\", log_buffer)\n",
        "    log_buffer = []\n",
        "accelerator.print(log_train)\n",
        "\n",
        "eval_loss, perplexity = evaluate()\n",
        "log_eval = {\n",
        "        \"loss/eval\": eval_loss,\n",
        "        \"perplexity\": perplexity,\n",
        "        \"completed_steps\": completed_steps,\n",
        "        \"lr\": lr_scheduler.get_lr(),\n",
        "        \"global_steps\" : global_steps,\n",
        "        \"epoch\": epoch,\n",
        "        \"steps\": num_steps_per_epoch + additional_steps - 1,\n",
        "        \"loss/train\": last_eval_log_train_loss,\n",
        "}\n",
        "accelerator.print(log_eval)\n",
        "save_results(drive_train_res_path, \"valid\", [log_eval])\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "unwrapped_model.save_pretrained(model_output_dir, save_function=accelerator.save)\n",
        "tokenizer.save_pretrained(model_output_dir)\n",
        "if accelerator.is_main_process:\n",
        "    if push_to_hub:\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Final model: completed_steps {completed_steps}; global_steps {global_steps};\\\n",
        "                             epoch {epoch}; steps {num_steps_per_epoch + additional_steps - 1}; lr {lr_scheduler.get_lr()};\\\n",
        "                             loss/eval {eval_loss}; perplexity {perplexity}; loss/train {last_eval_log_train_loss}\",\n",
        "            blocking=False\n",
        "        )\n",
        "\n",
        "model.train()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "CMxa25rIPbAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "add_to_tensorboard(drive_train_res_path, tensorboard_run_path)"
      ],
      "metadata": {
        "id": "fuzcHch1MWt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir \"{tensorboard_run_path}\""
      ],
      "metadata": {
        "id": "ccAmPDh7pwaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runtime.unassign()"
      ],
      "metadata": {
        "id": "VPbi9rSMe7L_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}